prp(
gerTree1,
extra = 1,
varlen = 0,
faclen = 0
)
set.seed(998) # Set a seed for reproducibility
data_prepost_ger[, 13:52]
data_prepost_ger[, 13:51]
data_prepost_ger[, 12:51]
data_prepost_ger[, 11:51]
participant_info <- read_delim(paste0(data,"ParticipantInfo_GERCAT.csv"), delim = ";")
# Load the information about duration of each segment (if needed)
data_df <- read.table(paste0(syllables, "fileDurationsDF.csv"), header = TRUE, sep = ',')
# Load cleaned syllable data
data <- read_csv(paste0(datasets, "data_cleaned.csv"))
# Load cleaned targets data
targets <- read_csv(paste0(datasets, "targets.csv"))
# Load cleaned targets with pre-post data
data_prepost <- read_csv(paste0(datasets, "data_prepost.csv"))
str(data_prepost)
data_prepost$percProm <- as.factor(data_prepost$percProm)
# First, remove the specified columns
data_prepost <- data_prepost %>%
select(-f1_freq_median, -f1_freq_median_norm, -f2_freq_median, -f2_freq_median_norm,
-f1_freq_medianPre, -f1_freq_median_normPre, -f2_freq_medianPre, -f2_freq_median_normPre,
-f1_freq_medianPost, -f1_freq_median_normPost, -f2_freq_medianPost, -f2_freq_median_normPost,
-pitch_sd, -pitch_median, -f0_slope, -pitch_sdPre, -pitch_medianPre, -f0_slopePre,
-pitch_sdPost, -pitch_medianPost, -f0_slopePost)
# Then, rearrange the remaining columns
data_prepost <- data_prepost %>%
select(fileName, language, participant, itemType, itemNum, focus, annotationNum,
annotationNumTarget, word, syllText, syllTextPre, syllTextPost, percProm,
duration, duration_noSilence, ampl_median, ampl_noSilence_median, env_slope,  pitch_median_norm,
pitch_sd_norm, f0_slope_norm, specCentroid_median, entropy_median, HNR_median, amEnvDep_median, fmDep_median, durationPre, duration_noSilencePre, ampl_medianPre, ampl_noSilence_medianPre, env_slopePre,
pitch_median_normPre, pitch_sd_normPre, f0_slope_normPre, specCentroid_medianPre, entropy_medianPre,
HNR_medianPre, amEnvDep_medianPre, fmDep_medianPre, durationPost, duration_noSilencePost, ampl_medianPost, ampl_noSilence_medianPost,
env_slopePost, pitch_median_normPost, pitch_sd_normPost, f0_slope_normPost,
specCentroid_medianPost, entropy_medianPost, HNR_medianPost, amEnvDep_medianPost, fmDep_medianPost)
View(data_prepost)
# Create data_prepost_german for rows where language is German
data_prepost_ger <- data_prepost %>%
filter(language == "German")
# Create data_prepost_catalan for rows where language is Catalan
data_prepost_cat <- data_prepost %>%
filter(language == "Catalan")
data_prepost_ger[, 13:52]
# First, exclude rows with NAs across columns 13 to 61 in data_prepost_ger because RF cannot deal with that
data_prepost_ger_clean <- data_prepost_ger[complete.cases(data_prepost_ger[, 13:52]), ]
# Split the data into training and testing subsets
sample_indices <- sample(1:nrow(data_prepost_ger_clean), 0.7*nrow(data_prepost_ger_clean)) # 70% training, 30% testing
train_data <- data_prepost_ger_clean[sample_indices, ]
test_data <- data_prepost_ger_clean[-sample_indices, ]
# Untuned Model with importance (permutation) option set
gerUntuned <- ranger(
y = train_data$percProm,
x = train_data[,13:52],
num.trees = 500,
importance = "permutation"
)
predictions <- predict(gerUntuned, data = test_data)$predictions
# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$percProm)
# Print the confusion matrix
print(confusion_matrix)
# Calculate feature importance
feature_importance <- importance(gerUntuned, num.threads = 1, type = 1)
# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")
# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]
# Print sorted feature importance
print(sorted_feature_importance)
# Untuned Model with importance (permutation) option set
gerUntuned <- ranger(
y = train_data$percProm,
x = train_data[,14:52],
num.trees = 500,
importance = "permutation"
)
predictions <- predict(gerUntuned, data = test_data)$predictions
# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$percProm)
# Print the confusion matrix
print(confusion_matrix)
# Calculate feature importance
feature_importance <- importance(gerUntuned, num.threads = 1, type = 1)
# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")
# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]
# Print sorted feature importance
print(sorted_feature_importance)
# Define the number of CPU cores to use
num_cores <- detectCores()
# Create a cluster with specified number of cores
cl <- makeCluster(num_cores)
# Close the cluster when you're done with your parallel tasks
stopCluster(cl)
tuneGer <- makeClassifTask(data = data_prepost_ger_clean[,13:52],
target = "percProm")
tuneGer <- tuneRanger(tuneGer,
measure = list(multiclass.brier),
num.trees = 500)
#Return hyperparameter values
tuneGer
gerTuned <- ranger(
y = train_data$percProm,
x = train_data[,14:52],
num.trees = 5000,
mtry = 10, # Set the recommended mtry value (number of features).
min.node.size = 4, # Set the recommended min.node.size value (number of samples before a node terminates).
sample.fraction = 0.6350144, # Set the recommended sample fraction value.(% of data for bagging).
importance = "permutation" # Permutation is a computationally intensive test.
)
predictions <- predict(gerTuned, data = test_data)$predictions
# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$percProm)
# Print the confusion matrix
print(confusion_matrix)
# Calculate feature importance
feature_importance <- importance(gerTuned, num.threads = 1, type = 1)
# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")
# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]
# Print sorted feature importance
print(sorted_feature_importance)
# First, impute missing values for columns 13 to 52 in data_prepost_ger
data_prepost_ger_clean <- data_prepost_ger
data_prepost_ger_clean[, 13:52] <- lapply(data_prepost_ger_clean[, 13:52], function(x) {
if (is.numeric(x)) {
ifelse(is.na(x), mean(x, na.rm = TRUE), x) # Impute with mean
} else {
ifelse(is.na(x), as.character(modes(x)), x) # Impute with mode for factors/characters
}
})
# Split the data into training and testing subsets
sample_indices <- sample(1:nrow(data_prepost_ger_clean), 0.7 * nrow(data_prepost_ger_clean)) # 70% training, 30% testing
train_data <- data_prepost_ger_clean[sample_indices, ]
test_data <- data_prepost_ger_clean[-sample_indices, ]
# First, impute missing values for columns 13 to 52 in data_prepost_ger
data_prepost_ger_clean <- data_prepost_ger
data_prepost_ger_clean[, 14:52] <- lapply(data_prepost_ger_clean[, 14:52], function(x) {
if (is.numeric(x)) {
ifelse(is.na(x), mean(x, na.rm = TRUE), x) # Impute with mean
} else {
ifelse(is.na(x), as.character(modes(x)), x) # Impute with mode for factors/characters
}
})
# Split the data into training and testing subsets
sample_indices <- sample(1:nrow(data_prepost_ger_clean), 0.7 * nrow(data_prepost_ger_clean)) # 70% training, 30% testing
train_data <- data_prepost_ger_clean[sample_indices, ]
test_data <- data_prepost_ger_clean[-sample_indices, ]
# Create a classification task for tuning
tuneGer <- makeClassifTask(data = train_data[, 13:52], target = "percProm")
# Tune the model
tuneGer <- tuneRanger(tuneGer, measure = list(multiclass.brier), num.trees = 500)
# Return hyperparameter values
tuneGer
# Fit the tuned model on the training data
gerTuned <- ranger(
y = train_data$percProm,
x = train_data[, 14:52],
num.trees = 5000,
mtry = 11,
min.node.size = 3,
sample.fraction = 0.4698381,
importance = "permutation"
)
# Predict on the test data
predictions <- predict(gerTuned, data = test_data)$predictions
# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$percProm)
# Print the confusion matrix
print(confusion_matrix)
# Calculate feature importance
feature_importance <- importance(gerTuned, num.threads = 1, type = 1)
# Convert to data frame
feature_importance_df <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance_df$Feature <- rownames(feature_importance_df)
colnames(feature_importance_df) <- c("Importance", "Feature")
# Sort by importance
sorted_feature_importance <- feature_importance_df[order(-feature_importance_df$Importance), ]
# Print sorted feature importance
print(sorted_feature_importance)
# Perform mice imputation
imputed_data <- mice(data_prepost_ger, m = 5, method = 'pmm', seed = 998)
# Complete the dataset
data_prepost_ger_clean <- complete(imputed_data)
train_data[, 12:52]
# Split the data into training and testing subsets
sample_indices <- sample(1:nrow(data_prepost_ger_clean), 0.7 * nrow(data_prepost_ger_clean)) # 70% training, 30% testing
train_data <- data_prepost_ger_clean[sample_indices, ]
test_data <- data_prepost_ger_clean[-sample_indices, ]
# Create a classification task for tuning
tuneGer <- makeClassifTask(data = train_data[, 13:52], target = "percProm")
# Tune the model
tuneGer <- tuneRanger(tuneGer, measure = list(multiclass.brier), num.trees = 500)
# Return hyperparameter values
tuneGer
# Fit the tuned model on the training data
gerTuned <- ranger(
y = train_data$percProm,
x = train_data[, 14:52],
num.trees = 5000,
mtry = 8,
min.node.size = 2,
sample.fraction = 0.5495003,
importance = "permutation"
)
# Predict on the test data
predictions <- predict(gerTuned, data = test_data)$predictions
# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$percProm)
# Print the confusion matrix
print(confusion_matrix)
# Calculate feature importance
feature_importance <- importance(gerTuned, num.threads = 1, type = 1)
# Convert to data frame
feature_importance_df <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance_df$Feature <- rownames(feature_importance_df)
colnames(feature_importance_df) <- c("Importance", "Feature")
# Sort by importance
sorted_feature_importance <- feature_importance_df[order(-feature_importance_df$Importance), ]
# Print sorted feature importance
print(sorted_feature_importance)
# Detect the number of available cores
cores <- detectCores() #- 1  # Leave one core free
# Create a cluster with the detected number of cores
cl <- makeCluster(cores)
# Register the parallel backend
registerDoParallel(cl)
grid_tune <- expand.grid(
nrounds = c(5000, 10000),
max_depth = c(3, 6),
eta = c(0.05, 0.1),
gamma = c(0.1),
colsample_bytree = c(0.6, 0.8),
min_child_weight = c(1),
subsample = c(0.75, 1.0)
)
# Calculate total combinations
total_combinations <- nrow(grid_tune)
# Estimate single model run time (assume 1 minute per run)
single_model_time <- 10 # minute
# Total runs for cross-validation
folds <- 5
total_runs <- total_combinations * folds
# Total time estimation without parallel processing
total_time <- total_runs * single_model_time # in minutes
# Convert to hours
total_time_hours <- total_time / 60
# Output estimated time without parallel processing
print(paste("Estimated time for grid search without parallel processing:", total_time_hours, "hours"))
# Parallel processing with 4 cores
cores <- 24
total_time_parallel <- total_time / cores # in minutes
# Convert to hours
total_time_parallel_hours <- total_time_parallel / 60
# Output estimated time with parallel processing
print(paste("Estimated time for grid search with", cores, "cores:", total_time_parallel_hours, "hours"))
rm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)
data_prepost_ger[,12:52]
# Set seed for reproducibility
set.seed(998)
# Set up train control
train_control <- trainControl(
method = "cv",        # Cross-validation
number = 5,           # 5-fold cross-validation
allowParallel = TRUE  # Enable parallel processing
)
# Define the number of subsets
numSubsets <- 5
# Create an empty list to store subsets
gerSubsets <- vector("list", length = numSubsets)
# only keep the columns of output and predictor variables
gerDataXGB <- data_prepost_ger[,13:52]
# Perform MICE imputation
imputed_data <- mice(gerDataXGB, m = 1, maxit = 50, method = 'pmm', seed = 500)
gerDataXGB <- complete(imputed_data)
# Calculate the number of samples in each subset
subsetSize <- nrow(gerDataXGB) %/% numSubsets
# Randomly assign samples to subsets
for (i in 1:numSubsets) {
if (i < numSubsets) {
gerSubsets[[i]] <- gerDataXGB[sample((1:nrow(gerDataXGB)), size = subsetSize), ]
} else {
gerSubsets[[i]] <- gerDataXGB[sample((1:nrow(gerDataXGB)), size = subsetSize + (nrow(gerDataXGB) %% numSubsets)), ]
}
}
# Naming the subsets
names(gerSubsets) <- paste0("gerData", 1:numSubsets)
# Access the subsets (e.g., gerData1, gerData2, etc.)
gerData1 <- gerSubsets$gerData1
gerData2 <- gerSubsets$gerData2
gerData3 <- gerSubsets$gerData3
gerData4 <- gerSubsets$gerData4
gerData5 <- gerSubsets$gerData5
# Combine subsets into 80% groups.
gerData1234 <- rbind(gerData1, gerData2, gerData3, gerData4)
gerData1235 <- rbind(gerData1, gerData2, gerData3, gerData5)
gerData1245 <- rbind(gerData1, gerData2, gerData4, gerData5)
gerData1345 <- rbind(gerData1, gerData3, gerData4, gerData5)
gerData2345 <- rbind(gerData2, gerData3, gerData4, gerData5)
gerModel1 <- caret::train(
percProm ~ .,
data = gerData1234,
method = "xgbTree",
trControl = train_control,
tuneGrid = grid_tune
)
saveRDS(gerModel1, file = paste0(models, "gerModel1.rds"), compress = TRUE)
gerModel2 <- caret::train(
percProm ~ .,
data = gerData1235,
method = "xgbTree",
trControl = train_control,
tuneGrid = grid_tune
)
saveRDS(gerModel2, file = paste0(models, "gerModel2.rds"), compress = TRUE)
gerModel3 <- caret::train(
percProm ~ .,
data = gerData1245,
method = "xgbTree",
trControl = train_control,
tuneGrid = grid_tune
)
saveRDS(gerModel3, file = paste0(models, "gerModel3.rds"), compress = TRUE)
gerModel4 <- caret::train(
percProm ~ .,
data = gerData1345,
method = "xgbTree",
trControl = train_control,
tuneGrid = grid_tune
)
saveRDS(gerModel4, file = paste0(models, "gerModel4.rds"), compress = TRUE)
gerModel5 <- caret::train(
percProm ~ .,
data = gerData2345,
method = "xgbTree",
trControl = train_control,
tuneGrid = grid_tune
)
saveRDS(gerModel5, file = paste0(models, "gerModel5.rds"), compress = TRUE)
# Generate predictions
predictions1 <- predict(gerModel1, newdata = gerData5)
predictions2 <- predict(gerModel2, newdata = gerData4)
predictions3 <- predict(gerModel3, newdata = gerData3)
predictions4 <- predict(gerModel4, newdata = gerData2)
predictions5 <- predict(gerModel5, newdata = gerData1)
# Compute confusion matrices
cm1 <- confusionMatrix(predictions1, gerData5$percProm)
cm2 <- confusionMatrix(predictions2, gerData4$percProm)
cm3 <- confusionMatrix(predictions3, gerData3$percProm)
cm4 <- confusionMatrix(predictions4, gerData2$percProm)
cm5 <- confusionMatrix(predictions5, gerData1$percProm)
# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)
p_values <- c(cm1$overall['AccuracyPValue'],
cm2$overall['AccuracyPValue'],
cm3$overall['AccuracyPValue'],
cm4$overall['AccuracyPValue'],
cm5$overall['AccuracyPValue'])
cm1
cm2
cm3
cm4
cm5
# Fisher's method
fisher_combined <- -2 * sum(log(p_values))
df <- 2 * length(p_values)
p_combined_fisher <- 1 - pchisq(fisher_combined, df)
print(p_combined_fisher)
p_combined_fisher
# Stouffer's method
z_scores <- qnorm(1 - p_values/2)
combined_z <- sum(z_scores) / sqrt(length(p_values))
p_combined_stouffer <- 2 * (1 - pnorm(abs(combined_z)))
print(p_combined_stouffer)
p_values
library(metap)
install.packages("metap")
library(metap)
# Fisher's method using metap package
fisher_result <- sumlog(p_values)
install.packages("multtest")
R.version
# Stouffer's method using metap package
stouffer_result <- sumz(p_values)
print(stouffer_result$p)
require(metap)
# Extract the xgboost model from the caret object
XGBgerModel1 <- gerModel1$finalModel
# Get the feature importance
importanceXGBgerModel1 <- xgb.importance(model = XGBgerModel1)
# Print the feature importance
print(importanceXGBgerModel1)
# Plot the feature importance
xgb.plot.importance(importanceXGBgerModel1)
XGBgerModel1 <- gerModel1$finalModel
importanceXGBgerModel1 <- xgb.importance(model = XGBgerModel1)
print(importanceXGBgerModel1)
xgb.plot.importance(importanceXGBgerModel1)
XGBgerModel2 <- gerModel2$finalModel
importanceXGBgerModel2 <- xgb.importance(model = XGBgerModel2)
print(importanceXGBgerModel2)
xgb.plot.importance(importanceXGBgerModel2)
XGBgerModel3 <- gerModel3$finalModel
importanceXGBgerModel3 <- xgb.importance(model = XGBgerModel3)
print(importanceXGBgerModel3)
xgb.plot.importance(importanceXGBgerModel3)
XGBgerModel4 <- gerModel4$finalModel
importanceXGBgerModel4 <- xgb.importance(model = XGBgerModel4)
print(importanceXGBgerModel4)
xgb.plot.importance(importanceXGBgerModel4)
XGBgerModel5 <- gerModel5$finalModel
importanceXGBgerModel5 <- xgb.importance(model = XGBgerModel5)
print(importanceXGBgerModel5)
xgb.plot.importance(importanceXGBgerModel5)
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
importance <- xgb.importance(model = model)
importance$Gain <- importance$Gain / sum(importance$Gain)
return(importance)
}
# Extract normalized importance for each model
importance1 <- get_normalized_importance(gerModel1$finalModel)
importance2 <- get_normalized_importance(gerModel2$finalModel)
importance3 <- get_normalized_importance(gerModel3$finalModel)
importance4 <- get_normalized_importance(gerModel4$finalModel)
importance5 <- get_normalized_importance(gerModel5$finalModel)
# Combine importances
all_importances <- list(importance1, importance2, importance3, importance4, importance5)
# Function to merge importances
merge_importances <- function(importances) {
merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
merged[is.na(merged)] <- 0  # Replace NAs with 0
merged$Cumulative <- rowSums(merged[, grep("Gain", colnames(merged))])
return(merged[, c("Feature", "Cumulative")])
}
# Merge and sort importances
cumulative_importance <- merge_importances(all_importances)
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
importance <- xgb.importance(model = model)
importance$Gain <- importance$Gain / sum(importance$Gain)
return(importance)
}
# Extract normalized importance for each model
importance1 <- get_normalized_importance(gerModel1$finalModel)
importance2 <- get_normalized_importance(gerModel2$finalModel)
importance3 <- get_normalized_importance(gerModel3$finalModel)
importance4 <- get_normalized_importance(gerModel4$finalModel)
importance5 <- get_normalized_importance(gerModel5$finalModel)
# Combine importances
all_importances <- list(importance1, importance2, importance3, importance4, importance5)
# Function to merge importances
merge_importances <- function(importances) {
for (i in 2:length(importances)) {
names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
}
merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
merged[is.na(merged)] <- 0  # Replace NAs with 0
gain_cols <- grep("Gain", colnames(merged))
merged$Cumulative <- rowSums(merged[, gain_cols])
return(merged[, c("Feature", "Cumulative")])
}
# Merge and sort importances
cumulative_importance <- merge_importances(all_importances)
cumulative_importance <- cumulative_importance[order(-cumulative_importance$Cumulative), ]
library(data.table)
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
importance <- xgb.importance(model = model)
importance$Gain <- importance$Gain / sum(importance$Gain)
return(importance)
}
# Extract normalized importance for each model
importance1 <- get_normalized_importance(gerModel1$finalModel)
importance2 <- get_normalized_importance(gerModel2$finalModel)
importance3 <- get_normalized_importance(gerModel3$finalModel)
importance4 <- get_normalized_importance(gerModel4$finalModel)
importance5 <- get_normalized_importance(gerModel5$finalModel)
# Combine importances
all_importances <- list(importance1, importance2, importance3, importance4, importance5)
# Function to merge importances
merge_importances <- function(importances) {
for (i in 2:length(importances)) {
names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
}
merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
merged[is.na(merged)] <- 0  # Replace NAs with 0
gain_cols <- grep("Gain", colnames(merged), value = TRUE)
merged$Cumulative <- rowSums(merged[, ..gain_cols])
return(merged[, .(Feature, Cumulative)])
}
# Merge and sort importances
cumulative_importance <- merge_importances(all_importances)
cumulative_importance <- cumulative_importance[order(-cumulative_importance$Cumulative), ]
# Print cumulative feature importance
print(cumulative_importance)
cm1
cm2
cm3
cm4
cm5
