# Save 'df' as a CSV file
write.csv(df, paste0(data, "df.csv"), row.names = FALSE)
View(df)
df <- df %>%
group_by(Language, Participant) %>%
mutate(env_mean_z = scale(env_mean),
env_range_z = scale(env_range),
env_min_z = scale(env_min),
env_max_z = scale(env_max),
env_slope_z = scale(env_slope),
F0_mean_z = scale(F0_mean),
F0_range_z = scale(F0_range),
F0_min_z = scale(F0_min),
F0_max_z = scale(F0_max),
F0_slope_z = scale(F0_slope))
df <- read.csv(paste0(data, "df.csv"))
# Update df with modified 'Syll' values
df <- df %>%
mutate(Syll = case_when(
Syll == "<p>Y" ~ "<p>",
Syll == "Pre" ~ "pre",
Syll == "Post" ~ "post",
Syll == "pre_post" ~ "post_pre",
Syll == "Pre_Post" ~ "post_pre",
Syll == "Post_Pre" ~ "post_pre",
Syll == "Post_pre" ~ "post_pre",
Syll == "post:pre" ~ "post_pre",
Syll == "post-pre" ~ "post_pre",
Syll == "postâ€“pre" ~ "post_pre",
Syll == "post_Pre" ~ "post_pre",
Syll == "prepost" ~ "post_pre",
Syll == "post_post" ~ "post_pre",
Syll == "post_pre " ~ "post_pre", # Remove trailing space
TRUE ~ Syll # Keep other values as they are
))
unique(df$Syll)
df <- df %>%
# Replace "N\t" with "N"
mutate(Word = gsub("N\\t", "N", Word)) %>%
# Change "NP" to "N" and "Adj" to "A"
mutate(Word = case_when(
Word == "NP" ~ "N",
Word == "Adj" ~ "A",
TRUE ~ Word
)) %>%
# Convert Word column to factor after all replacements
mutate(Word = as.factor(Word)) %>%
# Filter out specific rows based on Syll_num and File
filter(!(Syll_num == 8 & File == "C08_T_21_C") &
!(Syll_num == 2 & File == "G26_T_31_B"))
df <- df %>%
# First, handle the specific updates for Prosodic_Prom
mutate(# Ensure Prosodic_Prom is numeric before applying case_when
Prosodic_Prom = as.numeric(Prosodic_Prom),
Prosodic_Prom = case_when(
File == "G17_T_35_B" & Syll == "pfan" ~ 2,
File == "C02_T_13_I" & Syll == "lor_pre" ~ 1,
File == "C02_T_14_F" & Syll == "lor_pre" ~ 1,
TRUE ~ Prosodic_Prom
)) %>%
# Then, filter out the specific row to delete
# This case is arguably important, but since it is not the real target word, I delete it
filter(!(File == "G14_T_22_R" & Syll == "an"))
# Create subset without pre and post-tonic
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
# Columns to process
columns_to_process <- c("duration", "F0_mean", "F0_max", "F0_min", "F0_range",
"env_mean", "env_max", "env_min", "env_range"
)
# Function to calculate raw number and proportion of NAs
calculate_na_stats <- function(df, columns) {
na_counts <- colSums(is.na(df[, columns]))
total_counts <- nrow(df)
proportions <- na_counts / total_counts * 100
return(data.frame("NA_Count" = na_counts, "Proportion" = proportions))
}
# Initial NA stats
na_stats_before <- calculate_na_stats(df_targets, columns_to_process)
print(na_stats_before)
# Loop through each column and replace Inf and -Inf with NA
for (col_name in columns_to_process) {
# Replace Inf and -Inf with NA
df[[col_name]][df[[col_name]] == Inf | df[[col_name]] == -Inf] <- NA
}
# Process targets again
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
# NA stats after replacing Inf and -Inf
na_stats_after_replacement <- calculate_na_stats(df_targets, columns_to_process)
print(na_stats_after_replacement)
# Initialize a list to store outlier information
outliers_info <- list()
# Loop through each column
for (col_name in columns_to_process) {
cat("Processing column:", col_name, "\n")
# Calculate IQR
Q1 <- quantile(df[[col_name]], 0.25, na.rm = TRUE)
Q3 <- quantile(df[[col_name]], 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
# Define lower and upper bounds for outliers
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
# Identify outliers
outliers <- df[[col_name]] < lower_bound | df[[col_name]] > upper_bound
# Get outlier information
outliers_info[[col_name]] <- list(
N_outliers = sum(outliers, na.rm = TRUE),
Prop_outliers = (sum(outliers, na.rm = TRUE) / length(df[[col_name]])) * 100,
Mean_outliers = mean(df[[col_name]][outliers], na.rm = TRUE),
Mean_with_outliers = mean(df[[col_name]], na.rm = TRUE),
Mean_without_outliers = mean(df[[col_name]][!outliers], na.rm = TRUE)
)
# Replace outliers with NA
df[[col_name]][outliers] <- NA
}
# Create a summary table
outliers_summary <- data.frame(t(sapply(outliers_info, unlist)))
colnames(outliers_summary) <- c("N_outliers", "Prop_outliers", "Mean_outliers", "Mean_with_outliers", "Mean_without_outliers")
# Remove unnecessary variables
rm(outliers_info, Q1, Q3, lower_bound, upper_bound, IQR, col_name)
# Print the summary table
print(outliers_summary)
# Process targets again
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
# NA stats after outlier removal
na_stats_after_outliers <- calculate_na_stats(df_targets, columns_to_process)
print(na_stats_after_outliers)
rm(columns_to_process)
df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>"))) %>%
group_by(Language) %>%
summarize(Cumulative_Count = n())
df <- df %>%
group_by(Language, Participant) %>%
mutate(env_mean_z = scale(env_mean),
env_range_z = scale(env_range),
env_min_z = scale(env_min),
env_max_z = scale(env_max),
env_slope_z = scale(env_slope),
F0_mean_z = scale(F0_mean),
F0_range_z = scale(F0_range),
F0_min_z = scale(F0_min),
F0_max_z = scale(F0_max),
F0_slope_z = scale(F0_slope))
View(df)
=======
}
# With lines
for (var in variables) {
for (lang in languages) {
# Filter df_long for the current language and variable
data_filtered <- subset(df_long, variable == var & Language == lang)
# Generate the plot for the current language and variable
p <- ggplot(data = data_filtered, aes(x = phase, y = value, fill = phase)) +
geom_violin(scale = "width", trim = FALSE, alpha = 0.3) +
geom_boxplot(width = 0.1, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.5) +
geom_line(aes(group = interaction(File, Syll_num)), color = "grey", alpha = 0.1) +
scale_fill_manual(values = colorBlindBlack8) +
labs(x = "Syllable",
y = var,
title = lang) +
theme_minimal() +
theme(legend.position = "none")
# Dynamically generate the file name to include both language and variable
file_name <- paste0(plots, "prepost_", var, "_lines", "_", lang, ".pdf")
# Save the plot
ggsave(filename = file_name, plot = p, width = 10, height = 8)
}
}
rm(languages,lang,variables,var,data_filtered)
df_prepost_model <- df_prepost %>%
filter(!(Focus == "filler" |
(Word %in% c("NC", "PRON"))))
str(df_prepost_model)
df_prepost_model$Focus <- as.factor(df_prepost_model$Focus)
levels(df_prepost_model$Focus)
df_prepost_model$Focus <- factor(df_prepost_model$Focus, levels = c("background", "information", "contrastive", "corrective"))
table(df_prepost_model$Focus)
ggplot(df_prepost_model, aes(x = Focus)) +
geom_bar() +
labs(title = "Distribution of Focus Levels", x = "Focus", y = "Count") +
theme_minimal()
df_prepost_model <- df_prepost_model %>%
mutate(
Gender_s = if_else(Gender == "female", -0.5, 0.5),
Word_s = if_else(Word == "N", -0.5, 0.5)
)
mdl_focus_dur_noRS <- readRDS(paste0(models, "mdl_focus_dur_noRS.rds"))
summary(mdl_focus_dur_noRS)
transform_fixef <- function(model) {
# Extract fixed effects summary
fe_summary <- summary(model)$fixed
# Initialize a dataframe to store results
results <- data.frame(Term = rownames(fe_summary)[-1],
Estimate_corr = numeric(length(rownames(fe_summary)[-1])),
Est_Error_corr = numeric(length(rownames(fe_summary)[-1])),
Q2.5_corr = numeric(length(rownames(fe_summary)[-1])),
Q97.5_corr = numeric(length(rownames(fe_summary)[-1])), stringsAsFactors = FALSE)
# Loop through each fixed effect, excluding the intercept
for (i in 2:nrow(fe_summary)) {
#term <- fe_summary$term[i]
# Calculate transformations using column numbers for Q2.5 and Q97.5
estimate_in_hz <- exp(fe_summary[1,1]) * exp(fe_summary[i, 1]) - exp(fe_summary[1,1])
est_error_in_hz <- exp(fe_summary[1,1]) * exp(fe_summary[i, 2]) - exp(fe_summary[1,1])
q2_5_in_hz <- exp(fe_summary[1,1]) * exp(fe_summary[i, 3]) - exp(fe_summary[1,1])
q97_5_in_hz <- exp(fe_summary[1,1]) * exp(fe_summary[i, 4]) - exp(fe_summary[1,1])
# Update results dataframe directly without rbind to maintain the association between terms and their calculations
results$Estimate_corr[i-1] <- estimate_in_hz
results$Est_Error_corr[i-1] <- est_error_in_hz
results$Q2.5_corr[i-1] <- q2_5_in_hz
results$Q97.5_corr[i-1] <- q97_5_in_hz
}
return(results)
}
summary(mdl_focus_dur_noRS)
transform_fixef(mdl_focus_dur_noRS)
exp(fixef(mdl_focus_dur_noRS))
fixef(mdl_focus_dur_noRS)
exp(fixef(mdl_focus_dur_noRS))
transform_fixef(mdl_focus_dur_noRS)
# run loo mdl
if (file.exists(paste0(models, "mdl_focus_dur_noRS_loo.rds"))) {
mdl_focus_dur_noRS_loo <- readRDS(paste0(models, "mdl_focus_dur_noRS_loo.rds"))
} else {
mdl_focus_dur_noRS_loo <- loo(mdl_focus_dur_noRS)
saveRDS(mdl_focus_dur_noRS_loo, paste0(models, "mdl_focus_dur_noRS_loo.rds"))
}
mdl_focus_dur_noRS_int <- readRDS(paste0(models, "mdl_focus_dur_noRS_int.rds"))
# run loo mdl
if (file.exists(paste0(models, "mdl_focus_dur_noRS_int_loo.rds"))) {
mdl_focus_dur_noRS_int_loo <- readRDS(paste0(models, "mdl_focus_dur_noRS_int_loo.rds"))
} else {
mdl_focus_dur_noRS_int_loo <- loo(mdl_focus_dur_noRS_int)
saveRDS(mdl_focus_dur_noRS_int_loo, paste0(models, "mdl_focus_dur_noRS_int_loo.rds"))
}
mdl_focus_dur_noSlopes <- readRDS(paste0(models, "mdl_focus_dur_noSlopes.rds"))
# run loo mdl
if (file.exists(paste0(models, "mdl_focus_dur_noSlopes_loo.rds"))) {
mdl_focus_dur_noSlopes_loo <- readRDS(paste0(models, "mdl_focus_dur_noSlopes_loo.rds"))
} else {
mdl_focus_dur_noSlopes_loo <- loo(mdl_focus_dur_noSlopes)
saveRDS(mdl_focus_dur_noSlopes_loo, paste0(models, "mdl_focus_dur_noSlopes_loo.rds"))
}
mdl_focus_dur_noSlopes_int <- readRDS(paste0(models, "mdl_focus_dur_noSlopes_int.rds"))
# run loo mdl
if (file.exists(paste0(models, "mdl_focus_dur_noSlopes_int_loo.rds"))) {
mdl_focus_dur_noSlopes_int_loo <- readRDS(paste0(models, "mdl_focus_dur_noSlopes_int_loo.rds"))
} else {
mdl_focus_dur_noSlopes_int_loo <- loo(mdl_focus_dur_noSlopes_int)
saveRDS(mdl_focus_dur_noSlopes_int_loo, paste0(models, "mdl_focus_dur_noSlopes_int_loo.rds"))
}
mdl_focus_dur_full <- readRDS(paste0(models, "mdl_focus_dur_full.rds"))
# run loo mdl
if (file.exists(paste0(models, "mdl_focus_dur_full_loo.rds"))) {
mdl_focus_dur_full_loo <- readRDS(paste0(models, "mdl_focus_dur_full_loo.rds"))
} else {
mdl_focus_dur_full_loo <- loo(mdl_focus_dur_full)
saveRDS(mdl_focus_dur_full_loo, paste0(models, "mdl_focus_dur_full_loo.rds"))
}
mdl_focus_dur_full_int <- readRDS(paste0(models, "mdl_focus_dur_full_int.rds"))
# run loo mdl
if (file.exists(paste0(models, "mdl_focus_dur_full_int_loo.rds"))) {
mdl_focus_dur_full_int_loo <- readRDS(paste0(models, "mdl_focus_dur_full_int_loo.rds"))
} else {
mdl_focus_dur_full_int_loo <- loo(mdl_focus_dur_full_int)
saveRDS(mdl_focus_dur_full_int_loo, paste0(models, "mdl_focus_dur_full_int_loo.rds"))
}
comp_duration <- loo_compare(mdl_focus_dur_noRS_loo,
mdl_focus_dur_noRS_int_loo,
mdl_focus_dur_noSlopes_loo,
mdl_focus_dur_noSlopes_int_loo,
mdl_focus_dur_full_loo,
mdl_focus_dur_full_int_loo)
comp_duration
summary(mdl_focus_dur_full_int)
exp(fixef(mdl_focus_dur_full_int))
transform_fixef(mdl_focus_dur_full_int)
hypothesis(mdl_focus_dur_full_int, c("Focusinformation < 0",
"Focuscontrastive < 0",
"Focuscorrective < 0",
"Gender_s < 0",
"Word_s > 0",
"Focusinformation:Gender_s > 0",
"Focuscontrastive:Gender_s > 0",
"Focuscorrective:Gender_s < 0",
"Focusinformation:Word_s < 0",
"Focuscontrastive:Word_s > 0",
"Focuscorrective:Word_s > 0",
"Gender_s:Word_s > 0"))
plot(hypothesis(mdl_focus_dur_full_int, c("Focusinformation < 0",
"Focuscontrastive < 0",
"Focuscorrective < 0",
"Gender_s < 0",
"Word_s > 0",
"Focusinformation:Gender_s > 0",
"Focuscontrastive:Gender_s > 0",
"Focuscorrective:Gender_s < 0",
"Focusinformation:Word_s < 0",
"Focuscontrastive:Word_s > 0",
"Focuscorrective:Word_s > 0",
"Gender_s:Word_s > 0")))
post_focus_dur_full_int <- mdl_focus_dur_full_int %>%
gather_draws(`b_Focusinformation`,
`b_Focuscontrastive`,
`b_Focuscorrective`,
`b_Gender_s`,
`b_Word_s`,
`b_Focusinformation:Gender_s`,
`b_Focuscontrastive:Gender_s`,
`b_Focuscorrective:Gender_s`,
`b_Focusinformation:Word_s`,
`b_Focuscontrastive:Word_s`,
`b_Focuscorrective:Word_s`,
`b_Gender_s:Word_s`)
# Plotting intervals with densities
postplot_focus_dur_full_int <-
ggplot(post_focus_dur_full_int,
aes(x = .value, y = .variable)) +
stat_halfeye(.width = 0.95, fill = colorBlindBlack8[2], alpha = 0.7, size = 2) +
geom_segment(x = 0, xend = 0, y = -Inf, yend = Inf,
linetype = "dashed", color = "grey", size = 1) +
theme_bw() +
labs(x = "Posterior density", y = "Variable") +
scale_x_continuous(limits = c(-2, 1.5))
postplot_focus_dur_full_int
ggsave(filename = paste0(plots, "postplot_focus_dur_full_int.pdf"), plot = postplot_focus_dur_full_int, width = 8, height = 12)
rownames(summary(mdl_focus_dur_full_int)$fixed)
post_focus_dur_full_int <- mdl_focus_dur_full_int %>%
gather_draws(`b_Focusinformation`,
`b_Focuscontrastive`,
`b_Focuscorrective`,
`b_Gender_s`,
`b_Word_s`,
`b_Focusinformation:Gender_s`,
`b_Focuscontrastive:Gender_s`,
`b_Focuscorrective:Gender_s`,
`b_Focusinformation:Word_s`,
`b_Focuscontrastive:Word_s`,
`b_Focuscorrective:Word_s`,
`b_Gender_s:Word_s`)
# Plotting intervals with densities
postplot_focus_dur_full_int <-
ggplot(post_focus_dur_full_int,
aes(x = .value, y = .variable)) +
stat_halfeye(.width = 0.95, fill = colorBlindBlack8[2], alpha = 0.7, size = 2) +
geom_segment(x = 0, xend = 0, y = -Inf, yend = Inf,
linetype = "dashed", color = "grey", size = 1) +
theme_bw() +
labs(x = "Posterior density", y = "Variable") +
scale_x_continuous(limits = c(-2, 1.5))
postplot_focus_dur_full_int
ggsave(filename = paste0(plots, "postplot_focus_dur_full_int.pdf"), plot = postplot_focus_dur_full_int, width = 8, height = 12)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: source setup
########## folders ##########
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())
data          <- paste0(parentfolder, '/MultIS_data/')
audiodata     <- paste0(parentfolder, '/audio_processed/')
dataworkspace <- paste0(parentfolder, '/data_processed/')
datamerged    <- paste0(parentfolder, '/data_merged/')
models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
scripts       <- paste0(parentfolder, '/scripts/')
########## source file ##########
#source(paste0(scripts, "adjectives-preparation.R"))
#################### packages ####################
# audiovisual processing
library(seewave) # this for signal processing
library(signal)
library(rPraat)
library(dplR)
library(signal)
library(rstudioapi)
library(stringr)
library(wrassp)
library(readr)
library(readxl)
library(readtextgrid)   #reading txt grids
library(textgRid)
library(zoo)
library(pracma)
#library(stringi) # for syllable replacement
library(tuneR)
# data manipulation and viz
library(tidyverse)
library(ggpubr)
library(gridExtra)
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# modeling
library(broom) # for tidy model outputs
library(brms)
library(cmdstanr)
library(HDInterval) # package for credible interval computation
library(tidybayes) # plotting
library(bayesplot)
# option for Bayesian regression models:
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
# Bayes factor
#library(BayesFactor)
library(bayestestR)
# Diagnostics GUI
library(shinystan)
# Chunk 3: read metadata
participant_info <- read_delim(paste0(data,"ParticipantInfo_GERCAT.csv"), delim = ";")
# Chunk 4: locate wav
wavFiles <- list.files(data, pattern = "\\.wav$", full.names = TRUE)
# This is the tier we will be cutting from
targetTier <- "Label"
>>>>>>> Stashed changes
setwd("~/GitHub/Prominence_in_focus/Analysis/scripts")
########## folders ##########
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())
data          <- paste0(parentfolder, '/MultIS_data/')
audiodata     <- paste0(parentfolder, '/audio_processed/')
dataworkspace <- paste0(parentfolder, '/data_processed/')
datamerged    <- paste0(parentfolder, '/data_merged/')
models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
scripts       <- paste0(parentfolder, '/scripts/')
########## source file ##########
#source(paste0(scripts, "adjectives-preparation.R"))
#################### packages ####################
# audiovisual processing
library(seewave) # this for signal processing
library(signal)
library(rPraat)
library(dplR)
library(signal)
library(rstudioapi)
library(stringr)
library(wrassp)
library(readr)
library(readxl)
library(readtextgrid)   #reading txt grids
library(textgRid)
library(zoo)
library(pracma)
#library(stringi) # for syllable replacement
library(tuneR)
# data manipulation and viz
library(tidyverse)
library(ggpubr)
library(gridExtra)
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# modeling
library(broom) # for tidy model outputs
library(brms)
library(cmdstanr)
library(HDInterval) # package for credible interval computation
library(tidybayes) # plotting
library(bayesplot)
# option for Bayesian regression models:
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
# Bayes factor
#library(BayesFactor)
library(bayestestR)
# Diagnostics GUI
library(shinystan)
participant_info <- read_delim(paste0(data,"ParticipantInfo_GERCAT.csv"), delim = ";")
setwd("~/GitHub/Prominence_in_focus/Analysis/scripts")
########## folders ##########
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())
data          <- paste0(parentfolder, '/MultIS_data/')
audiodata     <- paste0(parentfolder, '/audio_processed/')
dataworkspace <- paste0(parentfolder, '/data_processed/')
datamerged    <- paste0(parentfolder, '/data_merged/')
models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
scripts       <- paste0(parentfolder, '/scripts/')
########## source file ##########
#source(paste0(scripts, "adjectives-preparation.R"))
#################### packages ####################
library(soundgen) # sound analysis
library(readr)    # data wrangling
library(tidyr)
library(dplyr)
library(umap) # umap
library(ggplot2) # plotting
library(viridis) # plotting
library(ggforce) #
library(plotly) # interactive plots
library(tidyverse)
library(ggpubr)
library(gridExtra)
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
participant_info <- read_delim(paste0(data,"ParticipantInfo_GERCAT.csv"), delim = ";")
# Generate the corresponding TextGrid file name
textGridFile <- sub("\\.wav$", ".TextGrid", wavFiles[1])
wavFiles <- list.files(data, pattern = "\\.wav$", full.names = TRUE)
# This is the tier we will be cutting from
targetTier <- "Label"
# Generate the corresponding TextGrid file name
textGridFile <- sub("\\.wav$", ".TextGrid", wavFiles[1])
# Read the TextGrid file
tg <- readtextgrid::read_textgrid(textGridFile)
tg <- readtextgrid::read_textgrid(textGridFile)
data
audiodata
wavFiles <- list.files(audiodata, pattern = "\\.wav$", full.names = TRUE)
# This is the tier we will be cutting from
targetTier <- "Label"
