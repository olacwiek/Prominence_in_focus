test$time_ms <= max_time]),
Syll_num = unique(test$Syll_num[test$time_ms >= min_time &
test$time_ms <= max_time &
!is.na(test$Syll_num)])
) %>%
left_join(test %>%
select(File, Participant, Language, Item_type, Item_num, Focus, Gender, Age, Syll, Syll_num) %>%
distinct(File, Syll_num, .keep_all = TRUE), by = "Syll_num")
# Append the result to the list
results_list[[csv_file]] <- test_intervals
}
}
warnings()
rm(csv_file, csv_files, test, test_intervals)
# Combine all results into a single dataframe
final_result <- do.call(rbind, results_list)
# Rearrange the column names
final_result <- final_result %>%
select(
"File", "Language", "Participant", "Item_type", "Item_num", "Focus", "TP", "Word", "Syll", "Syll_num", "Prosodic_Prom",
"min_time", "max_time", "F0_mean", "F0_min", "F0_max", "F0_range", "F0_slope", "env_mean", "env_min", "env_max", "env_range", "env_slope", "duration", "Gender", "Age"
)
df <- final_result %>%
dplyr::filter(Item_type == "target") #%>%
#subset(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "<p>") & is.na(Prosodic_Prom)))
# Initialize an empty data frame to store all txtgrds data
combined_txtgrds <- data.frame()
allfiles <- unique(df$File)
for (onefile in allfiles) {
fname <- paste0(audiodata, onefile, '.TextGrid')
txtgrds <- readtextgrid::read_textgrid(fname)
txtgrds <- txtgrds %>%
filter(tier_name == "Syll")
# Append the current txtgrds to the combined data frame
combined_txtgrds <- bind_rows(combined_txtgrds, txtgrds)
}
rm(allfiles, onefile)
# After the loop, you can perform the final_result manipulation
df <- df %>%
left_join(combined_txtgrds %>%
mutate(file = gsub(".TextGrid", "", file),
xmin = xmin * 1000,  # Convert seconds to milliseconds
xmax = xmax * 1000) %>%  # Convert seconds to milliseconds
select(file, annotation_num, xmin, xmax),
by = c("File" = "file", "Syll_num" = "annotation_num")) %>%
mutate(min_time = coalesce(xmin, min_time),  # Use xmin if available, else min_time
max_time = coalesce(xmax, max_time),  # Use xmax if available, else max_time
duration = ifelse(is.na(min_time) | is.na(max_time), NA, max_time - min_time)) %>%
select(-xmin, -xmax)  # Deselect specified variables
# Save 'df' as a CSV file
write.csv(df, paste0(data, "df.csv"), row.names = FALSE)
df <- read.csv(paste0(data, "df.csv"))
# Update df with modified 'Syll' values
df <- df %>%
mutate(Syll = case_when(
Syll == "<p>Y" ~ "<p>",
Syll == "Pre" ~ "pre",
Syll == "Post" ~ "post",
Syll == "pre_post" ~ "post_pre",
Syll == "Pre_Post" ~ "post_pre",
Syll == "Post_Pre" ~ "post_pre",
Syll == "Post_pre" ~ "post_pre",
Syll == "post:pre" ~ "post_pre",
Syll == "post-pre" ~ "post_pre",
Syll == "postâ€“pre" ~ "post_pre",
Syll == "post_Pre" ~ "post_pre",
Syll == "prepost" ~ "post_pre",
Syll == "post_post" ~ "post_pre",
Syll == "post_pre " ~ "post_pre", # Remove trailing space
TRUE ~ Syll # Keep other values as they are
))
unique(df$Syll)
df <- df %>%
# Replace "N\t" with "N"
mutate(Word = gsub("N\\t", "N", Word)) %>%
# Change "NP" to "N" and "Adj" to "A"
mutate(Word = case_when(
Word == "NP" ~ "N",
Word == "Adj" ~ "A",
TRUE ~ Word
)) %>%
# Convert Word column to factor after all replacements
mutate(Word = as.factor(Word)) %>%
# Filter out specific rows based on Syll_num and File
filter(!(Syll_num == 8 & File == "C08_T_21_C") &
!(Syll_num == 2 & File == "G26_T_31_B"))
df <- df %>%
# First, handle the specific updates for Prosodic_Prom
mutate(# Ensure Prosodic_Prom is numeric before applying case_when
Prosodic_Prom = as.numeric(Prosodic_Prom),
Prosodic_Prom = case_when(
File == "G17_T_35_B" & Syll == "pfan" ~ 2,
File == "C02_T_13_I" & Syll == "lor_pre" ~ 1,
File == "C02_T_14_F" & Syll == "lor_pre" ~ 1,
TRUE ~ Prosodic_Prom
)) %>%
# Then, filter out the specific row to delete
# This case is arguably important, but since it is not the real target word, I delete it
filter(!(File == "G14_T_22_R" & Syll == "an"))
# Create subset without pre and post-tonic
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
# Columns to process
columns_to_process <- c("duration", "F0_mean", "F0_max", "F0_min", "F0_range",
"env_mean", "env_max", "env_min", "env_range"
)
# Function to calculate raw number and proportion of NAs
calculate_na_stats <- function(df, columns) {
na_counts <- colSums(is.na(df[, columns]))
total_counts <- nrow(df)
proportions <- na_counts / total_counts * 100
return(data.frame("NA_Count" = na_counts, "Proportion" = proportions))
}
# Initial NA stats
na_stats_before <- calculate_na_stats(df_targets, columns_to_process)
print(na_stats_before)
# Loop through each column and replace Inf and -Inf with NA
for (col_name in columns_to_process) {
# Replace Inf and -Inf with NA
df[[col_name]][df[[col_name]] == Inf | df[[col_name]] == -Inf] <- NA
}
# Process targets again
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
# NA stats after replacing Inf and -Inf
na_stats_after_replacement <- calculate_na_stats(df_targets, columns_to_process)
print(na_stats_after_replacement)
# Initialize a list to store outlier information
outliers_info <- list()
# Loop through each column
for (col_name in columns_to_process) {
cat("Processing column:", col_name, "\n")
# Calculate IQR
Q1 <- quantile(df[[col_name]], 0.25, na.rm = TRUE)
Q3 <- quantile(df[[col_name]], 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
# Define lower and upper bounds for outliers
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
# Identify outliers
outliers <- df[[col_name]] < lower_bound | df[[col_name]] > upper_bound
# Get outlier information
outliers_info[[col_name]] <- list(
N_outliers = sum(outliers, na.rm = TRUE),
Prop_outliers = (sum(outliers, na.rm = TRUE) / length(df[[col_name]])) * 100,
Mean_outliers = mean(df[[col_name]][outliers], na.rm = TRUE),
Mean_with_outliers = mean(df[[col_name]], na.rm = TRUE),
Mean_without_outliers = mean(df[[col_name]][!outliers], na.rm = TRUE)
)
# Replace outliers with NA
df[[col_name]][outliers] <- NA
}
# Create a summary table
outliers_summary <- data.frame(t(sapply(outliers_info, unlist)))
colnames(outliers_summary) <- c("N_outliers", "Prop_outliers", "Mean_outliers", "Mean_with_outliers", "Mean_without_outliers")
# Remove unnecessary variables
rm(outliers_info, Q1, Q3, lower_bound, upper_bound, IQR, col_name)
# Print the summary table
print(outliers_summary)
# Process targets again
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
# NA stats after outlier removal
na_stats_after_outliers <- calculate_na_stats(df_targets, columns_to_process)
print(na_stats_after_outliers)
rm(columns_to_process)
df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>"))) %>%
group_by(Language) %>%
summarize(Cumulative_Count = n())
syll_per_foc <-
df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>"))) %>%
group_by(Language, Focus) %>%
summarize(Count = n()) %>%
mutate(Proportion = Count / sum(Count))
syll_per_foc
## Count
ggplot(syll_per_foc, aes(x = Focus, y = Count, fill = Language)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
labs(#title = "Count of focus types",
x = "Focus", y = "Count") +
scale_fill_manual(values = colorBlindBlack8) +
theme_minimal()
ggsave(filename = paste0(plots, "focus_count.pdf"), plot = last_plot(), width = 6, height = 4)
## Proportion
ggplot(syll_per_foc, aes(x = Focus, y = Proportion, fill = Language)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
labs(#title = "Proportion of focus types",
x = "Focus", y = "Proportion") +
scale_fill_manual(values = colorBlindBlack8) +
theme_minimal()
ggsave(filename = paste0(plots, "focus_prop.pdf"), plot = last_plot(), width = 6, height = 4)
syll_per_pros <-
df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>"))) %>%
group_by(Language, Prosodic_Prom) %>%
summarize(Count = n()) %>%
mutate(Proportion = Count / sum(Count))
syll_per_pros
# Identify NA
df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>"))) %>%
filter(Language == "German", is.na(Prosodic_Prom)) %>%
select(File, Syll)
df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>"))) %>%
filter(Language == "Catalan", is.na(Prosodic_Prom)) %>%
select(File, Syll)
## Count
ggplot(syll_per_pros, aes(x = Prosodic_Prom, y = Count, fill = Language)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
labs(#title = "Count of Syll per Language and Prominence",
x = "Perceived prominence", y = "Count") +
scale_fill_manual(values = colorBlindBlack8) +
theme_minimal()
ggsave(filename = paste0(plots, "prominence_count.pdf"), plot = last_plot(), width = 6, height = 4)
## Proportion
ggplot(syll_per_pros, aes(x = Prosodic_Prom, y = Proportion, fill = Language)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
labs(#title = "Proportion of Syll per Language and Prominence",
x = "Perceived prominence", y = "Proportion") +
scale_fill_manual(values = colorBlindBlack8) +
theme_minimal()
ggsave(filename = paste0(plots, "prominence_prop.pdf"), plot = last_plot(), width = 6, height = 4)
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>"))) %>%
group_by(Language, Prosodic_Prom) %>%
summarize(Average_Duration = mean(duration, na.rm = TRUE))
ggplot(df_targets %>% filter(!is.na(Prosodic_Prom)), aes(x = Language, y = duration, fill = as.factor(Prosodic_Prom))) +
geom_violin(scale = "width", trim = FALSE, alpha = 0.3) +
geom_boxplot(width = 0.1, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.5) +
labs(
#title = "Duration of Prosodic Prominence Ratings by Language",
x = "Language",
y = "Duration",
fill = "Prosodic prominence"
) +
scale_fill_manual(values = colorBlindBlack8) +
theme_minimal()
ggsave(filename = paste0(plots, "prominence_duration.pdf"), plot = last_plot(), width = 6, height = 4)
df$Focus <- factor(df$Focus, levels = c("background", "information", "filler", "contrastive", "corrective"))
df_targets$Focus <- factor(df_targets$Focus, levels = c("background", "information", "filler", "contrastive", "corrective"))
df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>"))) %>%
distinct(Item_type) %>%
pull(Item_type)
df <- df %>%
group_by(Language, Participant) %>%
mutate(env_mean_z = scale(env_mean),
env_range_z = scale(env_range),
env_min_z = scale(env_min),
env_max_z = scale(env_max),
env_slope_z = scale(env_slope),
F0_mean_z = scale(F0_mean),
F0_range_z = scale(F0_range),
F0_min_z = scale(F0_min),
F0_max_z = scale(F0_max),
F0_slope_z = scale(F0_slope))
str(df)
View(df)
# Get a list of all CSV files in the datamerged directory
csv_files <- list.files(datamerged, pattern = ".csv", full.names = TRUE)
# Initialize an empty list to store results
results_list <- list()
mode_function <- function(x) {
uniq_x <- na.omit(x)  # Remove NAs
if (length(uniq_x) == 0) {
return(NA)  # Return NA if all values are NA
}
counts <- table(uniq_x)
max_count <- max(counts)
mode_vals <- names(counts[counts == max_count])
if (length(mode_vals) > 1) {
mode_val <- mode_vals[1]  # Select the first mode in case of ties
} else {
mode_val <- mode_vals
}
if (is.na(mode_val) && sum(is.na(x)) > max_count) {
return(NA)  # Return NA if NAs are more frequent
}
return(mode_val)
}
# Loop through each CSV file and process it
for (csv_file in csv_files) {
# Read the CSV file
test <- read.csv(csv_file)
# Filter and summarize Syll
test_intervals <-
test %>%
filter(!is.na(Syll) & Syll != "") %>%
group_by(Syll_num) %>%
summarize(min_time = min(time_ms),
max_time = max(time_ms)) %>%
arrange(min_time)
# Check if test_intervals has rows
if (nrow(test_intervals) > 0) {
# Calculate additional metrics
test_intervals <- test_intervals %>%
rowwise() %>% # Ensure calculations are performed for each row
mutate(
F0_min = min(test$F0[test$time_ms >= min_time & test$time_ms <= max_time & test$F0 > 0], na.rm = TRUE),
F0_max = max(test$F0[test$time_ms >= min_time & test$time_ms <= max_time & test$F0 > 0], na.rm = TRUE),
F0_mean = mean(test$F0[test$time_ms >= min_time & test$time_ms <= max_time & test$F0 > 0], na.rm = TRUE),
F0_range = ifelse(is.na(F0_min) | is.na(F0_max), NA, F0_max - F0_min),
F0_slope = { temp_data <- test[test$time_ms >= min_time & test$time_ms <= max_time & test$F0 > 0, ];
if(nrow(temp_data) > 1) coef(lm(F0 ~ time_ms, data = temp_data))[2] else NA },
env_min = min(test$env[test$time_ms >= min_time & test$time_ms <= max_time], na.rm = TRUE),
env_max = max(test$env[test$time_ms >= min_time & test$time_ms <= max_time], na.rm = TRUE),
env_mean = mean(test$env[test$time_ms >= min_time & test$time_ms <= max_time], na.rm = TRUE),
env_range = ifelse(is.na(env_min) | is.na(env_max), NA, env_max - env_min),
env_slope = { temp_data <- test[test$time_ms >= min_time & test$time_ms <= max_time, ];
if(nrow(temp_data) > 1) coef(lm(env ~ time_ms, data = temp_data))[2] else NA },
duration = ifelse(is.na(min_time) | is.na(max_time), NA, max_time - min_time),
Prosodic_Prom =
mode_function(test$Prosodic_Prom[test$time_ms >= min_time & test$time_ms <= max_time]),
TP =
mode_function(test$TP[test$time_ms >= min_time & test$time_ms <= max_time]),
Word =
mode_function(test$Word[test$time_ms >= min_time & test$time_ms <= max_time])
) %>%
left_join(test %>%
select(File, Participant, Language, Item_type, Item_num, Focus, Gender, Age, Syll, Syll_num) %>%
distinct(File, Syll_num, .keep_all = TRUE), by = "Syll_num")
# Append the result to the list
results_list[[csv_file]] <- test_intervals
}
}
rm(csv_file, csv_files, test, test_intervals)
# Combine all results into a single dataframe
final_result <- do.call(rbind, results_list)
# Rearrange the column names
final_result <- final_result %>%
select(
"File", "Language", "Participant", "Item_type", "Item_num", "Focus", "TP", "Word", "Syll", "Syll_num", "Prosodic_Prom",
"min_time", "max_time", "F0_mean", "F0_min", "F0_max", "F0_range", "F0_slope", "env_mean", "env_min", "env_max", "env_range", "env_slope", "duration", "Gender", "Age"
)
df <- final_result %>%
dplyr::filter(Item_type == "target") #%>%
# Initialize an empty data frame to store all txtgrds data
combined_txtgrds <- data.frame()
allfiles <- unique(df$File)
for (onefile in allfiles) {
fname <- paste0(audiodata, onefile, '.TextGrid')
txtgrds <- readtextgrid::read_textgrid(fname)
txtgrds <- txtgrds %>%
filter(tier_name == "Syll")
# Append the current txtgrds to the combined data frame
combined_txtgrds <- bind_rows(combined_txtgrds, txtgrds)
}
rm(allfiles, onefile)
# After the loop, you can perform the final_result manipulation
df <- df %>%
left_join(combined_txtgrds %>%
mutate(file = gsub(".TextGrid", "", file),
xmin = xmin * 1000,  # Convert seconds to milliseconds
xmax = xmax * 1000) %>%  # Convert seconds to milliseconds
select(file, annotation_num, xmin, xmax),
by = c("File" = "file", "Syll_num" = "annotation_num")) %>%
mutate(min_time = coalesce(xmin, min_time),  # Use xmin if available, else min_time
max_time = coalesce(xmax, max_time),  # Use xmax if available, else max_time
duration = ifelse(is.na(min_time) | is.na(max_time), NA, max_time - min_time)) %>%
select(-xmin, -xmax)  # Deselect specified variables
# Save 'df' as a CSV file
write.csv(df, paste0(data, "df.csv"), row.names = FALSE)
View(df)
df <- df %>%
group_by(Language, Participant) %>%
mutate(env_mean_z = scale(env_mean),
env_range_z = scale(env_range),
env_min_z = scale(env_min),
env_max_z = scale(env_max),
env_slope_z = scale(env_slope),
F0_mean_z = scale(F0_mean),
F0_range_z = scale(F0_range),
F0_min_z = scale(F0_min),
F0_max_z = scale(F0_max),
F0_slope_z = scale(F0_slope))
df <- read.csv(paste0(data, "df.csv"))
# Update df with modified 'Syll' values
df <- df %>%
mutate(Syll = case_when(
Syll == "<p>Y" ~ "<p>",
Syll == "Pre" ~ "pre",
Syll == "Post" ~ "post",
Syll == "pre_post" ~ "post_pre",
Syll == "Pre_Post" ~ "post_pre",
Syll == "Post_Pre" ~ "post_pre",
Syll == "Post_pre" ~ "post_pre",
Syll == "post:pre" ~ "post_pre",
Syll == "post-pre" ~ "post_pre",
Syll == "postâ€“pre" ~ "post_pre",
Syll == "post_Pre" ~ "post_pre",
Syll == "prepost" ~ "post_pre",
Syll == "post_post" ~ "post_pre",
Syll == "post_pre " ~ "post_pre", # Remove trailing space
TRUE ~ Syll # Keep other values as they are
))
unique(df$Syll)
df <- df %>%
# Replace "N\t" with "N"
mutate(Word = gsub("N\\t", "N", Word)) %>%
# Change "NP" to "N" and "Adj" to "A"
mutate(Word = case_when(
Word == "NP" ~ "N",
Word == "Adj" ~ "A",
TRUE ~ Word
)) %>%
# Convert Word column to factor after all replacements
mutate(Word = as.factor(Word)) %>%
# Filter out specific rows based on Syll_num and File
filter(!(Syll_num == 8 & File == "C08_T_21_C") &
!(Syll_num == 2 & File == "G26_T_31_B"))
df <- df %>%
# First, handle the specific updates for Prosodic_Prom
mutate(# Ensure Prosodic_Prom is numeric before applying case_when
Prosodic_Prom = as.numeric(Prosodic_Prom),
Prosodic_Prom = case_when(
File == "G17_T_35_B" & Syll == "pfan" ~ 2,
File == "C02_T_13_I" & Syll == "lor_pre" ~ 1,
File == "C02_T_14_F" & Syll == "lor_pre" ~ 1,
TRUE ~ Prosodic_Prom
)) %>%
# Then, filter out the specific row to delete
# This case is arguably important, but since it is not the real target word, I delete it
filter(!(File == "G14_T_22_R" & Syll == "an"))
# Create subset without pre and post-tonic
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
# Columns to process
columns_to_process <- c("duration", "F0_mean", "F0_max", "F0_min", "F0_range",
"env_mean", "env_max", "env_min", "env_range"
)
# Function to calculate raw number and proportion of NAs
calculate_na_stats <- function(df, columns) {
na_counts <- colSums(is.na(df[, columns]))
total_counts <- nrow(df)
proportions <- na_counts / total_counts * 100
return(data.frame("NA_Count" = na_counts, "Proportion" = proportions))
}
# Initial NA stats
na_stats_before <- calculate_na_stats(df_targets, columns_to_process)
print(na_stats_before)
# Loop through each column and replace Inf and -Inf with NA
for (col_name in columns_to_process) {
# Replace Inf and -Inf with NA
df[[col_name]][df[[col_name]] == Inf | df[[col_name]] == -Inf] <- NA
}
# Process targets again
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
# NA stats after replacing Inf and -Inf
na_stats_after_replacement <- calculate_na_stats(df_targets, columns_to_process)
print(na_stats_after_replacement)
# Initialize a list to store outlier information
outliers_info <- list()
# Loop through each column
for (col_name in columns_to_process) {
cat("Processing column:", col_name, "\n")
# Calculate IQR
Q1 <- quantile(df[[col_name]], 0.25, na.rm = TRUE)
Q3 <- quantile(df[[col_name]], 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
# Define lower and upper bounds for outliers
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
# Identify outliers
outliers <- df[[col_name]] < lower_bound | df[[col_name]] > upper_bound
# Get outlier information
outliers_info[[col_name]] <- list(
N_outliers = sum(outliers, na.rm = TRUE),
Prop_outliers = (sum(outliers, na.rm = TRUE) / length(df[[col_name]])) * 100,
Mean_outliers = mean(df[[col_name]][outliers], na.rm = TRUE),
Mean_with_outliers = mean(df[[col_name]], na.rm = TRUE),
Mean_without_outliers = mean(df[[col_name]][!outliers], na.rm = TRUE)
)
# Replace outliers with NA
df[[col_name]][outliers] <- NA
}
# Create a summary table
outliers_summary <- data.frame(t(sapply(outliers_info, unlist)))
colnames(outliers_summary) <- c("N_outliers", "Prop_outliers", "Mean_outliers", "Mean_with_outliers", "Mean_without_outliers")
# Remove unnecessary variables
rm(outliers_info, Q1, Q3, lower_bound, upper_bound, IQR, col_name)
# Print the summary table
print(outliers_summary)
# Process targets again
df_targets <- df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>")))
# NA stats after outlier removal
na_stats_after_outliers <- calculate_na_stats(df_targets, columns_to_process)
print(na_stats_after_outliers)
rm(columns_to_process)
df %>%
filter(!(Syll %in% c("pre", "post", "", "post_pre", "disfluency", "break", "<p>"))) %>%
group_by(Language) %>%
summarize(Cumulative_Count = n())
df <- df %>%
group_by(Language, Participant) %>%
mutate(env_mean_z = scale(env_mean),
env_range_z = scale(env_range),
env_min_z = scale(env_min),
env_max_z = scale(env_max),
env_slope_z = scale(env_slope),
F0_mean_z = scale(F0_mean),
F0_range_z = scale(F0_range),
F0_min_z = scale(F0_min),
F0_max_z = scale(F0_max),
F0_slope_z = scale(F0_slope))
View(df)
