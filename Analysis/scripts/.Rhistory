tg$tier_name
tg
writeTextGrid <- function(data, outputFile) {
# Open a connection to the output file
con <- file(outputFile, "w")
# Write the TextGrid header
cat('File type = "ooTextFile"\n', file = con)
cat('Object class = "TextGrid"\n', file = con)
cat('\n', file = con)  # Add an extra line
cat('xmin = 0 \n', file = con)
cat('xmax =', max(data$xmax), '\n', file = con)
cat('tiers? <exists> \n', file = con)
cat('size =', n_distinct(data$tier_name), '\n', file = con)
cat('item []:', '\n', file = con)
# Write the tier information
tiers <- unique(data[, c("tier_num", "tier_name", "tier_type")])
for (i in 1:nrow(tiers)) {
cat(paste0('    item [', i, ']:\n'), file = con)
cat(paste0('        class = "', tiers$tier_type[i], '"\n'), file = con)
cat(paste0('        name = "', tiers$tier_name[i], '"\n'), file = con)
cat(paste0('        xmin = 0\n'), file = con)
cat(paste0('        xmax = ', max(data$xmax), '\n'), file = con)
# Write intervals or points
if (tiers$tier_type[i] == "IntervalTier") {
intervals <- data[data$tier_name == tiers$tier_name[i], ]
cat(paste0('        intervals: size = ', nrow(intervals), '\n'), file = con)
for (j in 1:nrow(intervals)) {
cat(paste0('        intervals [', j, ']:\n'), file = con)
cat(paste0('            xmin = ', intervals$xmin[j], '\n'), file = con)
cat(paste0('            xmax = ', intervals$xmax[j], '\n'), file = con)
cat(paste0('            text = "', intervals$text[j], '"\n'), file = con)
}
} else if (tiers$tier_type[i] == "PointTier") {
points <- data[data$tier_name == tiers$tier_name[i], ]
cat(paste0('        points: size = ', nrow(points), '\n'), file = con)
for (j in 1:nrow(points)) {
cat(paste0('        points [', j, ']:\n'), file = con)
cat(paste0('            number = ', j - 1, '\n'), file = con)  # Point numbers start from 0
cat(paste0('            time = ', points$xmax[j], '\n'), file = con)  # Assuming xmax as time
cat(paste0('            mark = "', points$text[j], '"\n'), file = con)
}
}
}
# Close the connection
close(con)
}
# Loop through each file
for (textGrid in files) {
# Read the TextGrid file
tg <- readtextgrid::read_textgrid(textGrid)
# Change 'silences' in tier_name to 'vocalization'
tg$tier_name[tg$tier_name == "silences"] <- "vocalization"
# Create a new row to be added
new_row <- tibble(
file = unique(tg$file),
tier_num = max(tg$tier_num) + 1,
tier_name = "gesture",
tier_type = unique(tg$tier_type),
tier_xmin = 0,
tier_xmax = max(tg$tier_xmax),
xmin = 0,
xmax = max(tg$xmax),
text = "no_gesture",
annotation_num = 1
)
# Append the new row to the tibble
tg <- bind_rows(tg, new_row)
# Save the modified TextGrid back to the same file
writeTextGrid(tg, textGrid)
}
files_avi <- list.files(parentfolder, pattern = "\\.avi$", full.names = TRUE)
files
files_avi
head(files)
head(files_avi)
# Create csv for readme on annotating process
# Create a data frame
data <- data.frame(
files = files,
files_avi = files_avi,
comments = rep("", length(files))  # Empty comments column
)
write.csv(data, paste0(parentfolder, "file_comments.csv"), row.names = FALSE)
write.csv(data, paste0(parentfolder, "/file_comments.csv"), row.names = FALSE)
# Create csv for readme on annotating process
# Create a data frame
data <- data.frame(
files = basename(files),
files_avi = basenames(files_avi),
comments = rep("", length(files))  # Empty comments column
)
# Create csv for readme on annotating process
# Create a data frame
data <- data.frame(
files = basename(files),
files_avi = basename(files_avi),
comments = rep("", length(files))  # Empty comments column
)
View(data)
write.csv(data, paste0(parentfolder, "/file_comments.csv"), row.names = FALSE)
# Create a data frame for comments on the files
data <- data.frame(
files = sub("\\.TextGrid$", "", basename(files)),
files_avi = sub("\\.avi$", "", basename(files_avi)),
condition <- ifelse(grepl("geluiden", files), "vocalization",
ifelse(grepl("combinatie", files), "combined",
ifelse(grepl("gebaren", files), "gesture", NA))),
comments_audio = rep("", length(files)),
comments_video = rep("", length(files))
)
View(data)
# Create a data frame for comments on the files
data <- data.frame(
files = sub("\\.TextGrid$", "", basename(files)),
files_avi = sub("\\.avi$", "", basename(files_avi)),
condition = ifelse(grepl("geluiden", files), "vocalization",
ifelse(grepl("combinatie", files), "combined",
ifelse(grepl("gebaren", files), "gesture", NA))),
comments_audio = rep("", length(files)),
comments_video = rep("", length(files))
)
View(data)
write.csv(data, paste0(parentfolder, "/file_comments.csv"), row.names = FALSE)
install.packages("writexl")
library(writexlsx)
library(writexl)
write_xlsx(data, paste0(parentfolder, "/file_comments.xlsx"), row.names = FALSE)
write_xlsx(data, paste0(parentfolder, "/file_comments.xlsx"))
new_names <- sub("\\.avi$", ".TextGrid", basename(files_avi))
new_names_textGrid <- sub("\\.avi$", ".TextGrid", basename(files_avi))
new_names_wav <- sub("\\.avi$", ".wav", basename(files_avi))
new_paths <- file.path(dirname(files_avi), new_names_textGrid)
new_paths
new_paths_textGrid <- file.path(dirname(files_avi), new_names_textGrid)
new_paths_wav <- file.path(dirname(files_avi), new_names_wav)
files
results <- file.rename(files, new_paths_textGrid)
files_wav <- list.files(parentfolder, pattern = "\\.wav$", full.names = TRUE)
results <- file.rename(files_wav, new_paths_wav)
files_wav <- list.files(parentfolder, pattern = "\\.wav$", full.names = TRUE)
files_wav
files_wav <- list.files(parentfolder, pattern = "\\.wav$", full.names = TRUE)
files_wav
results <- file.rename(files_wav, new_paths_wav)
rm(results, new_names, new_names_textGrid, new_names_wav, new_paths, new_paths_textGrid, new_paths_wav, files, files_avi, files_wav)
raw_files <- list.files(directory_path, pattern = "_video_raw.*$", full.names = TRUE)
raw_files <- list.files(parentfolder, pattern = "_video_raw.*$", full.names = TRUE)
head(raw_files)
new_names <- sub("_video_raw(\\.[^.]+)$", "\\1", raw_files)
results <- file.rename(raw_files, new_names)
rm(raw_files, new_names, results)
files_avi <- list.files(parentfolder, pattern = "\\.avi$", full.names = TRUE)
data <- data.frame(
file = sub("\\.avi$", "", basename(files_avi)),
condition = ifelse(grepl("geluiden", files), "vocalization",
ifelse(grepl("combinatie", files), "combined",
ifelse(grepl("gebaren", files), "gesture", NA))),
comments_audio = rep("", length(files)),
comments_video = rep("", length(files))
)
data <- data.frame(
file = sub("\\.avi$", "", basename(files_avi)),
condition = ifelse(grepl("geluiden", files_avi), "vocalization",
ifelse(grepl("combinatie", files_avi), "combined",
ifelse(grepl("gebaren", files_avi), "gesture", NA))),
comments_audio = rep("", length(files_avi)),
comments_video = rep("", length(files_avi))
)
write_xlsx(data, paste0(parentfolder, "/file_comments.xlsx"))
setwd("~/GitHub/Prominence_in_focus/Analysis/scripts")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: source setup
########## folders ##########
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())
data          <- paste0(parentfolder, '/MultIS_data/')
audiodata     <- paste0(parentfolder, '/audio_processed/')
syllables     <- paste0(audiodata,    'syllables/')
dataworkspace <- paste0(parentfolder, '/data_processed/')
datamerged    <- paste0(parentfolder, '/data_merged/')
datasets      <- paste0(parentfolder, '/datasets/')
models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
scripts       <- paste0(parentfolder, '/scripts/')
########## source file ##########
#source(paste0(scripts, "adjectives-preparation.R"))
#################### packages ####################
# Data Manipulation
library(tibble)
library(stringr)
library(tidyverse) # includes readr, tidyr, dplyr, ggplot2
# Plotting
library(ggforce)
library(ggpubr)
library(gridExtra)
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# Chunk 3: read metadata
participant_info <- read_delim(paste0(data,"ParticipantInfo_GERCAT.csv"), delim = ";")
# Load the information about duration of each segment (if needed)
data_df <- read.table(paste0(syllables, "fileDurationsDF.csv"), header = TRUE, sep = ',')
# Load cleaned syllable data
data <- read_csv(paste0(datasets, "data_cleaned.csv"))
# Load cleaned targets data
targets <- read_csv(paste0(datasets, "targets.csv"))
# Load cleaned targets with pre-post data
data_prepost <- read_csv(paste0(datasets, "data_prepost.csv"))
# Chunk 4: metadata merge
# Process participant_info so that participant number column is only number
participant_info$Participant <- parse_number(participant_info$Participant)
# Merge the dataframes by "Participant" and "Language"
# Exchange META to the dataframe of your liking
# META <- merge(META, participant_info, by = c("Participant", "Language"), all.x = TRUE)
# Chunk 5: calculate NAs
# Columns to process
columns_to_process <- c(
"duration", "duration_noSilence", "ampl_median", "ampl_noSilence_median", "env_slope",
"pitch_median", "pitch_sd", "f0_slope", "f1_freq_median", "f2_freq_median",
"specCentroid_median", "entropy_median", "HNR_median", "amEnvDep_median", "fmDep_median",
"pitch_median_norm", "pitch_sd_norm", "f0_slope_norm", "f1_freq_median_norm", "f2_freq_median_norm",
"syllTextPre", "durationPre", "duration_noSilencePre", "ampl_medianPre",
"ampl_noSilence_medianPre", "env_slopePre", "pitch_medianPre", "pitch_sdPre", "f0_slopePre",
"f1_freq_medianPre", "f2_freq_medianPre", "specCentroid_medianPre", "entropy_medianPre", "HNR_medianPre",
"amEnvDep_medianPre", "fmDep_medianPre", "pitch_median_normPre", "pitch_sd_normPre", "f0_slope_normPre",
"f1_freq_median_normPre", "f2_freq_median_normPre", "syllTextPost", "durationPost", "duration_noSilencePost",
"ampl_medianPost", "ampl_noSilence_medianPost", "env_slopePost", "pitch_medianPost", "pitch_sdPost", "f0_slopePost",
"f1_freq_medianPost", "f2_freq_medianPost", "specCentroid_medianPost", "entropy_medianPost", "HNR_medianPost",
"amEnvDep_medianPost", "fmDep_medianPost", "pitch_median_normPost", "pitch_sd_normPost", "f0_slope_normPost",
"f1_freq_median_normPost", "f2_freq_median_normPost"
)
# Ensure columns to process are numeric
columns_to_process <- columns_to_process[columns_to_process %in% names(data_prepost)]
columns_to_process <- columns_to_process[sapply(data_prepost[columns_to_process], is.numeric)]
# Function to calculate raw number and proportion of NAs
calculate_na_stats <- function(df, columns) {
na_counts <- colSums(is.na(df[, columns]))
total_counts <- nrow(df)
proportions <- na_counts / total_counts * 100
return(data.frame("NA_Count" = na_counts, "Proportion" = proportions))
}
# Initial NA stats
na_stats_before <- calculate_na_stats(data_prepost, columns_to_process)
print(na_stats_before)
# Chunk 6: sylls per lang
targets %>%
group_by(language) %>%
summarize(Cumulative_Count = n())
targets %>%
group_by(language, percProm) %>%
summarize(avg_duration = mean(duration, na.rm = TRUE))
ggplot(targets %>% filter(!is.na(percProm)), aes(x = language, y = duration, fill = as.factor(percProm))) +
geom_violin(scale = "width", trim = FALSE, alpha = 0.3) +
geom_boxplot(width = 0.1, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.5) +
labs(
#title = "Duration of Prosodic Prominence Ratings by Language",
x = "Language",
y = "Duration (total)",
fill = "Prosodic prominence"
) +
scale_fill_manual(values = colorBlindBlack8) +
theme_minimal()
ggsave(filename = paste0(plots, "prominence_duration.pdf"), plot = last_plot(), width = 6, height = 4)
ggplot(targets %>% filter(!is.na(percProm)), aes(x = language, y = duration_noSilence, fill = as.factor(percProm))) +
geom_violin(scale = "width", trim = FALSE, alpha = 0.3) +
geom_boxplot(width = 0.1, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.5) +
labs(
#title = "Duration of Prosodic Prominence Ratings by Language",
x = "Language",
y = "Duration (without silences)",
fill = "Prosodic prominence"
) +
scale_fill_manual(values = colorBlindBlack8) +
theme_minimal()
ggsave(filename = paste0(plots, "prominence_duration_noSilence.pdf"), plot = last_plot(), width = 6, height = 4)
ggplot(targets %>% filter(!is.na(percProm)), aes(x = language, y = ampl_median, fill = as.factor(percProm))) +
geom_violin(scale = "width", trim = FALSE, alpha = 0.3) +
geom_boxplot(width = 0.1, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.5) +
labs(
#title = "Duration of Prosodic Prominence Ratings by Language",
x = "Language",
y = "Amplitude (medians)",
fill = "Prosodic prominence"
) +
scale_fill_manual(values = colorBlindBlack8) +
theme_minimal()
ggsave(filename = paste0(plots, "prominence_ampl_median.pdf"), plot = last_plot(), width = 6, height = 4)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: source setup
########## folders ##########
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())
data          <- paste0(parentfolder, '/MultIS_data/')
audiodata     <- paste0(parentfolder, '/audio_processed/')
syllables     <- paste0(audiodata,    'syllables/')
dataworkspace <- paste0(parentfolder, '/data_processed/')
datamerged    <- paste0(parentfolder, '/data_merged/')
datasets      <- paste0(parentfolder, '/datasets/')
models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
scripts       <- paste0(parentfolder, '/scripts/')
########## source file ##########
#source(paste0(scripts, "adjectives-preparation.R"))
#################### packages ####################
# Data Manipulation
library(tibble)
library(stringr)
library(tidyverse) # includes readr, tidyr, dplyr, ggplot2
library(data.table)
# Plotting
library(ggforce)
library(ggpubr)
library(gridExtra)
# Random Forests and XGBoost
library(rpart)
library(rpart.plot)
library(ranger)
library(tuneRanger)
library(caret)
library(xgboost)
library(parallel)
library(mice)
library(doParallel)
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# Chunk 3: read metadata
participant_info <- read_delim(paste0(data,"ParticipantInfo_GERCAT.csv"), delim = ";")
# Load the information about duration of each segment (if needed)
data_df <- read.table(paste0(syllables, "fileDurationsDF.csv"), header = TRUE, sep = ',')
# Load cleaned syllable data
data <- read_csv(paste0(datasets, "data_cleaned.csv"))
# Load cleaned targets data
targets <- read_csv(paste0(datasets, "targets.csv"))
# Load cleaned targets with pre-post data
data_prepost <- read_csv(paste0(datasets, "data_prepost.csv"))
str(data_prepost)
# QUESTION TO THINK ABOUT WHETHER percProm is factor????
data_prepost$percProm <- as.factor(data_prepost$percProm)
# First, remove the specified columns
data_prepost <- data_prepost %>%
select(-f1_freq_median, -f1_freq_median_norm, -f2_freq_median, -f2_freq_median_norm,
-f1_freq_medianPre, -f1_freq_median_normPre, -f2_freq_medianPre, -f2_freq_median_normPre,
-f1_freq_medianPost, -f1_freq_median_normPost, -f2_freq_medianPost, -f2_freq_median_normPost,
-pitch_sd, -pitch_median, -f0_slope, -pitch_sdPre, -pitch_medianPre, -f0_slopePre,
-pitch_sdPost, -pitch_medianPost, -f0_slopePost)
# Then, rearrange the remaining columns
data_prepost <- data_prepost %>%
select(fileName, language, participant, itemType, itemNum, focus, annotationNum,
annotationNumTarget, word, syllText, syllTextPre, syllTextPost, percProm,
duration, duration_noSilence, ampl_median, ampl_noSilence_median, env_slope, pitch_median_norm,
pitch_sd_norm, f0_slope_norm, specCentroid_median, entropy_median, HNR_median, amEnvDep_median, fmDep_median,
durationPre, duration_noSilencePre, ampl_medianPre, ampl_noSilence_medianPre, env_slopePre,
pitch_median_normPre, pitch_sd_normPre, f0_slope_normPre, specCentroid_medianPre, entropy_medianPre,
HNR_medianPre, amEnvDep_medianPre, fmDep_medianPre, durationPost, duration_noSilencePost, ampl_medianPost, ampl_noSilence_medianPost,
env_slopePost, pitch_median_normPost, pitch_sd_normPost, f0_slope_normPost,
specCentroid_medianPost, entropy_medianPost, HNR_medianPost, amEnvDep_medianPost, fmDep_medianPost)
# Create data_prepost_german for rows where language is German
data_prepost_ger <- data_prepost %>%
filter(language == "German")
# Create data_prepost_catalan for rows where language is Catalan
data_prepost_cat <- data_prepost %>%
filter(language == "Catalan")
set.seed(998) # Set a seed for reproducibility
# First, exclude rows with NAs across columns 13 to 61 in data_prepost_ger because RF cannot deal with that
data_prepost_ger_clean <- data_prepost_ger[complete.cases(data_prepost_ger[, 13:52]), ]
# Split the data into training and testing subsets
sample_indices <- sample(1:nrow(data_prepost_ger_clean), 0.7*nrow(data_prepost_ger_clean)) # 70% training, 30% testing
train_data <- data_prepost_ger_clean[sample_indices, ]
test_data <- data_prepost_ger_clean[-sample_indices, ]
# Define the number of CPU cores to use
num_cores <- detectCores()
# Create a cluster with specified number of cores
cl <- makeCluster(num_cores)
# Close the cluster when you're done with your parallel tasks
stopCluster(cl)
# Perform mice imputation
imputed_data <- mice(data_prepost_ger, m = 5, method = 'pmm', seed = 998)
# Complete the dataset
data_prepost_ger_clean <- complete(imputed_data)
grid_tune <- expand.grid(
nrounds = c(5000, 10000),
max_depth = c(3, 6),
eta = c(0.05, 0.1),
gamma = c(0.1),
colsample_bytree = c(0.6, 0.8),
min_child_weight = c(1),
subsample = c(0.75, 1.0)
)
# Set seed for reproducibility
set.seed(998)
# Set up train control
train_control <- trainControl(
method = "cv",        # Cross-validation
number = 5,           # 5-fold cross-validation
allowParallel = TRUE  # Enable parallel processing
)
# Define the number of subsets
numSubsets <- 5
# Create an empty list to store subsets
gerSubsets <- vector("list", length = numSubsets)
# only keep the columns of output and predictor variables
gerDataXGB <- data_prepost_ger[,13:52]
# Perform MICE imputation
imputed_data <- mice(gerDataXGB, m = 1, maxit = 50, method = 'pmm', seed = 500)
gerDataXGB <- complete(imputed_data)
# Calculate the number of samples in each subset
subsetSize <- nrow(gerDataXGB) %/% numSubsets
# Randomly assign samples to subsets
for (i in 1:numSubsets) {
if (i < numSubsets) {
gerSubsets[[i]] <- gerDataXGB[sample((1:nrow(gerDataXGB)), size = subsetSize), ]
} else {
gerSubsets[[i]] <- gerDataXGB[sample((1:nrow(gerDataXGB)), size = subsetSize + (nrow(gerDataXGB) %% numSubsets)), ]
}
}
# Naming the subsets
names(gerSubsets) <- paste0("gerData", 1:numSubsets)
# Access the subsets (e.g., gerData1, gerData2, etc.)
gerData1 <- gerSubsets$gerData1
gerData2 <- gerSubsets$gerData2
gerData3 <- gerSubsets$gerData3
gerData4 <- gerSubsets$gerData4
gerData5 <- gerSubsets$gerData5
# Combine subsets into 80% groups.
gerData1234 <- rbind(gerData1, gerData2, gerData3, gerData4)
gerData1235 <- rbind(gerData1, gerData2, gerData3, gerData5)
gerData1245 <- rbind(gerData1, gerData2, gerData4, gerData5)
gerData1345 <- rbind(gerData1, gerData3, gerData4, gerData5)
gerData2345 <- rbind(gerData2, gerData3, gerData4, gerData5)
gerModel1 <- readRDS(paste0(models, "gerModel1.rds"))
gerModel2 <- readRDS(paste0(models, "gerModel2.rds"))
gerModel3 <- readRDS(paste0(models, "gerModel3.rds"))
gerModel4 <- readRDS(paste0(models, "gerModel4.rds"))
gerModel5 <- readRDS(paste0(models, "gerModel5.rds"))
# Generate predictions
predictions1 <- predict(gerModel1, newdata = gerData5)
predictions2 <- predict(gerModel2, newdata = gerData4)
predictions3 <- predict(gerModel3, newdata = gerData3)
predictions4 <- predict(gerModel4, newdata = gerData2)
predictions5 <- predict(gerModel5, newdata = gerData1)
# Compute confusion matrices
cm1 <- confusionMatrix(predictions1, gerData5$percProm)
cm2 <- confusionMatrix(predictions2, gerData4$percProm)
cm3 <- confusionMatrix(predictions3, gerData3$percProm)
cm4 <- confusionMatrix(predictions4, gerData2$percProm)
cm5 <- confusionMatrix(predictions5, gerData1$percProm)
# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)
p_values <- c(cm1$overall['AccuracyPValue'],
cm2$overall['AccuracyPValue'],
cm3$overall['AccuracyPValue'],
cm4$overall['AccuracyPValue'],
cm5$overall['AccuracyPValue'])
cm1
predictions1
cm1
cm2
cm3
cm4
cm5
XGBgerModel1 <- gerModel1$finalModel
importanceXGBgerModel1 <- xgb.importance(model = XGBgerModel1)
print(importanceXGBgerModel1)
xgb.plot.importance(importanceXGBgerModel1)
XGBgerModel2 <- gerModel2$finalModel
importanceXGBgerModel2 <- xgb.importance(model = XGBgerModel2)
print(importanceXGBgerModel2)
xgb.plot.importance(importanceXGBgerModel2)
XGBgerModel3 <- gerModel3$finalModel
importanceXGBgerModel3 <- xgb.importance(model = XGBgerModel3)
print(importanceXGBgerModel3)
xgb.plot.importance(importanceXGBgerModel3)
XGBgerModel4 <- gerModel4$finalModel
importanceXGBgerModel4 <- xgb.importance(model = XGBgerModel4)
print(importanceXGBgerModel4)
xgb.plot.importance(importanceXGBgerModel4)
XGBgerModel5 <- gerModel5$finalModel
importanceXGBgerModel5 <- xgb.importance(model = XGBgerModel5)
print(importanceXGBgerModel5)
xgb.plot.importance(importanceXGBgerModel5)
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
importance <- xgb.importance(model = model)
importance$Gain <- importance$Gain / sum(importance$Gain)
return(importance)
}
# Extract normalized importance for each model
importance1 <- get_normalized_importance(gerModel1$finalModel)
importance2 <- get_normalized_importance(gerModel2$finalModel)
importance3 <- get_normalized_importance(gerModel3$finalModel)
importance4 <- get_normalized_importance(gerModel4$finalModel)
importance5 <- get_normalized_importance(gerModel5$finalModel)
# Combine importances
all_importances <- list(importance1, importance2, importance3, importance4, importance5)
# Function to merge importances
merge_importances <- function(importances) {
for (i in 2:length(importances)) {
names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
}
merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
merged[is.na(merged)] <- 0  # Replace NAs with 0
gain_cols <- grep("Gain", colnames(merged), value = TRUE)
merged$Cumulative <- rowSums(merged[, ..gain_cols])
return(merged[, .(Feature, Cumulative)])
}
# Merge and sort importances
cumulative_importance <- merge_importances(all_importances)
cumulative_importance <- cumulative_importance[order(-cumulative_importance$Cumulative), ]
# Print cumulative feature importance
print(cumulative_importance)
