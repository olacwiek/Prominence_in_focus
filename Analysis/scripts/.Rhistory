if (!file.exists(outputFile)) {
# Save to the merge folder
write.csv(ac_merged, outputFile)
} else {
cat("File", outputFileName, "already exists in datamerged directory. Skipping.\n")
}
# Remove unnecessary dependencies
rm(env, F0, txtgrds, annotations, ac_merged, annotation, anno, anno_num, d, fname, outputFile, outputFileName, envelopedata, tier_name, trial, xmin, xmax, wavFile, wavFiles, txtgrd_tier)
}
}
rm(tiers_to_extract, cols_to_load)
# Get a list of all CSV files in the datamerged directory
csv_files <- list.files(datamerged, pattern = ".csv", full.names = TRUE)
# Initialize an empty list to store results
results_list <- list()
# Create a custom function to calculate the mode:
# The most frequent value within an interval
mode_function <- function(x) {
uniq_x <- na.omit(x)  # Remove NAs
if (length(uniq_x) == 0) {
return(NA)  # Return NA if all values are NA
}
counts <- table(uniq_x)
max_count <- max(counts)
mode_val <- names(counts[counts == max_count])
if (is.na(mode_val) && sum(is.na(x)) > max_count) {
return(NA)  # Return NA if NAs are more frequent
}
return(mode_val)
}
# Loop through each CSV file and process it
for (csv_file in csv_files) {
# Read the CSV file
test <- read.csv(csv_file)
# Filter and summarize Syll
test_intervals <-
test %>%
dplyr::filter(!is.na(Syll) & Syll != "") %>%
group_by(Syll_num) %>%
summarize(min_time = min(time_ms),
max_time = max(time_ms)) %>%
arrange(min_time)
# Check if test_intervals has rows
if (nrow(test_intervals) > 0) {
# Calculate additional metrics
test_intervals <- test_intervals %>%
group_by(Syll_num) %>%
mutate(
F0_min = min(test$F0[test$time_ms >= min_time &
test$time_ms <= max_time &
test$F0 > 0], na.rm = TRUE),
F0_max = max(test$F0[test$time_ms >= min_time &
test$time_ms <= max_time &
test$F0 > 0], na.rm = TRUE),
F0_mean = mean(test$F0[test$time_ms >= min_time &
test$time_ms <= max_time &
test$F0 > 0], na.rm = TRUE),
F0_range = ifelse(is.na(F0_min) | is.na(F0_max), NA, F0_max - F0_min),
env_min = min(test$env[test$time_ms >= min_time &
test$time_ms <= max_time], na.rm = TRUE),
env_max = max(test$env[test$time_ms >= min_time &
test$time_ms <= max_time], na.rm = TRUE),
env_mean = mean(test$env[test$time_ms >= min_time &
test$time_ms <= max_time], na.rm = TRUE),
env_range = ifelse(is.na(env_min) | is.na(env_max), NA, env_max - env_min),
duration = ifelse(is.na(min_time) | is.na(max_time), NA, max_time - min_time),
Prosodic_Prom =
mode_function(test$Prosodic_Prom[test$time_ms >= min_time &
test$time_ms <= max_time]),
TP =
mode_function(test$TP[test$time_ms >= min_time &
test$time_ms <= max_time]),
Word =
mode_function(test$Word[test$time_ms >= min_time &
test$time_ms <= max_time]),
Syll_num = unique(test$Syll_num[test$time_ms >= min_time &
test$time_ms <= max_time &
!is.na(test$Syll_num)])
) %>%
left_join(test %>%
select(File, Participant, Language, Item_type, Item_num, Focus, Gender, Age, Syll, Syll_num) %>%
distinct(File, Syll_num, .keep_all = TRUE), by = "Syll_num")
# Append the result to the list
results_list[[csv_file]] <- test_intervals
}
}
participant_info <- read_delim(paste0(data,"ParticipantInfo_GERCAT.csv"), delim = ";")
View(participant_info)
# List of the WAV files
list_wavs <- list.files(audiodata, pattern = ".wav")
# Function to map focus characters to focus values
mapFocus <- function(char) {
switch(char,
I = "information",
C = "contrastive",
R = "corrective",
B = "background",
F = "filler",
"unknown")
}
# Initialize an empty dataframe to store the metadata
META <- data.frame(File = character(),
Language = character(),
Participant = numeric(),
Item_type = character(),
Item_num = numeric(),
Focus = character(),
stringsAsFactors = FALSE)
# Loop through the WAV file names and decode the information
for (wavFile in list_wavs) {
# Split the file name using "_"
parts <- strsplit(wavFile, "_")[[1]]
# Extract the relevant parts
File <- sub("\\.wav$", "", wavFile)
Language <- if (startsWith(parts[1], "G")) "German" else "Catalan"
Participant <- as.numeric(parse_number(parts[1]))
Item_type <- if (parts[2] == "P") "practice" else "target"
Item_num <- as.numeric(parts[3])
Focus <- mapFocus(substr(parts[4], 1, 1))  # Extract the first character
# Create a data frame for the current file
file_meta <- data.frame(File, Language, Participant, Item_type, Item_num, Focus)
# Append the file metadata to META
META <- rbind(META, file_meta)
}
# Remove unnecessary variables from memory
rm(parts, File, Language, Participant, Item_type, Item_num, Focus, file_meta, wavFile, list_wavs)
View(META)
# List of the WAV files
list_wavs <- list.files(audiodata, pattern = ".wav")
# Function to map focus characters to focus values
mapFocus <- function(char) {
switch(char,
I = "information",
C = "contrastive",
R = "corrective",
B = "background",
F = "filler",
"unknown")
}
# Initialize an empty dataframe to store the metadata
META <- data.frame(File = character(),
Language = character(),
Participant = numeric(),
Item_type = character(),
Item_num = numeric(),
Focus = character(),
stringsAsFactors = FALSE)
# Loop through the WAV file names and decode the information
for (wavFile in list_wavs) {
# Split the file name using "_"
parts <- strsplit(wavFile, "_")[[1]]
# Extract the relevant parts
File <- sub("\\.wav$", "", wavFile)
Language <- if (startsWith(parts[1], "G")) "German" else "Catalan"
Participant <- as.numeric(parse_number(parts[1]))
Item_type <- if (parts[2] == "P") "practice" else "target"
Item_num <- as.numeric(parts[3])
Focus <- mapFocus(substr(parts[4], 1, 1))  # Extract the first character
# Create a data frame for the current file
file_meta <- data.frame(File, Language, Participant, Item_type, Item_num, Focus)
# Append the file metadata to META
META <- rbind(META, file_meta)
}
# Remove unnecessary variables from memory
rm(parts, File, Language, Participant, Item_type, Item_num, Focus, file_meta, wavFile, list_wavs)
View(META)
# Process participant_info so that participant number column is only number
participant_info$Participant <- parse_number(participant_info$Participant)
# Merge the dataframes by "Participant" and "Language"
META <- merge(META, participant_info, by = c("Participant", "Language"), all.x = TRUE)
View(META)
# First give the list of files for which we have envelopes
envelopedata <- list.files(dataworkspace, pattern = '*.ENV.csv')
############### butter filter ###############
butter.it <- function(x, samplingrate = 100, order = 2, lowpasscutoff = 30) {
bf <- butter(order, lowpasscutoff / samplingrate, type = "low") #normalized frequency
x <<- as.numeric(signal::filtfilt(bf, x)) #apply forwards and backwards using filtfilt
}
############### Select tiers and cols ##############
# List of tier names you want to extract
tiers_to_extract <- c("TP", "Word", "Gestural_Prom", "Prosodic_Prom", "Syll")
cols_to_load <- c("Participant", "Language", "Item_type", "Item_num", "Focus", "Gender", "Age")
############### Merging ###################
# Loop through envelope data files and merge
for (d in envelopedata) {
# trial <- str_remove(d, '_ENV.csv') # If you want to remove '_ENV.csv' from the file name
# Read envelope data
env <- read.csv(file.path(dataworkspace, d))
# Read F0 data
trial <- sub("_ENV.csv$", "", d) # Extract trial name from envelope data file name
F0 <- read.csv(file.path(dataworkspace, paste0(trial, '_F0.csv')))
# Read text grid information and prepare for reading into the time series
fname <- file.path(audiodata, paste0(trial, '.TextGrid'))
if (file.exists(fname)) {
# Prepare text grids
txtgrds <- readtextgrid::read_textgrid(fname)
# Initialize a list to store the annotations for each tier
annotations <- list()
# Loop through the tiers you want to extract
for (tier_name in tiers_to_extract) {
# Filter for the current tier
txtgrd_tier <- txtgrds[txtgrds$tier_name == tier_name, ]
# Extract necessary information
xmin <- round(txtgrd_tier$xmin * 1000)
xmax <- round(txtgrd_tier$xmax * 1000)
anno <- txtgrd_tier$text
anno_num <- txtgrd_tier$annotation_num
# Create the annotation data frame
annotation <- data.frame(xmin, xmax, anno, anno_num)
# Add it to the list with a consistent name
annotations[[tier_name]] <- annotation
}
# Merge envelope and F0 data
ac_merged <- merge(x = env, y = F0, by.x = "time_ms", by.y = "time_ms", all = TRUE)
ac_merged$env <- na.approx(ac_merged$env, x = ac_merged$time_ms, na.rm = FALSE)
# Filter rows where F0 is NA
ac_merged <- ac_merged[(!is.na(ac_merged$F0)),]
# Also remove trailing NA's
ac_merged <- na.trim(ac_merged)
# Loop through the tiers you want to extract and load them into ac_merged
for (tier_name in tiers_to_extract) {
if (tier_name == "Syll") {
# If the tier is "Syll," load numeric annotations
ac_merged$Syll_num <- load.in.anno_num(time_original = ac_merged$time_ms, numeric_annotations = annotations[[tier_name]])
# Also use load.in.event for "Syll" to store annotations
ac_merged[[tier_name]] <- load.in.event(time_original = ac_merged$time_ms, annotations = annotations[[tier_name]])
} else {
# For other tiers, use load.in.event
ac_merged[[tier_name]] <- load.in.event(time_original = ac_merged$time_ms, annotations = annotations[[tier_name]])
}
}
# Add a "File" column with the value of "trial"
ac_merged$File <- trial
# Add some info for analyses
ac_merged <- merge(ac_merged, META[, c("File", cols_to_load)], by = "File", all.x = TRUE)
# Check if the file already exists in the datamerged directory
outputFileName <- paste0(trial, '.csv')
outputFile <- file.path(datamerged, outputFileName)
if (!file.exists(outputFile)) {
# Save to the merge folder
write.csv(ac_merged, outputFile)
} else {
cat("File", outputFileName, "already exists in datamerged directory. Skipping.\n")
}
# Remove unnecessary dependencies
rm(env, F0, txtgrds, annotations, ac_merged, annotation, anno, anno_num, d, fname, outputFile, outputFileName, envelopedata, tier_name, trial, xmin, xmax, wavFile, wavFiles, txtgrd_tier)
}
}
View(META)
# First give the list of files for which we have envelopes
envelopedata <- list.files(dataworkspace, pattern = '*.ENV.csv')
############### butter filter ###############
butter.it <- function(x, samplingrate = 100, order = 2, lowpasscutoff = 30) {
bf <- butter(order, lowpasscutoff / samplingrate, type = "low") #normalized frequency
x <<- as.numeric(signal::filtfilt(bf, x)) #apply forwards and backwards using filtfilt
}
############### Select tiers and cols ##############
# List of tier names you want to extract
tiers_to_extract <- c("TP", "Word", "Gestural_Prom", "Prosodic_Prom", "Syll")
cols_to_load <- c("Participant", "Language", "Item_type", "Item_num", "Focus", "Gender", "Age", "Native_language")
############### Merging ###################
# Loop through envelope data files and merge
for (d in envelopedata) {
# trial <- str_remove(d, '_ENV.csv') # If you want to remove '_ENV.csv' from the file name
# Read envelope data
env <- read.csv(file.path(dataworkspace, d))
# Read F0 data
trial <- sub("_ENV.csv$", "", d) # Extract trial name from envelope data file name
F0 <- read.csv(file.path(dataworkspace, paste0(trial, '_F0.csv')))
# Read text grid information and prepare for reading into the time series
fname <- file.path(audiodata, paste0(trial, '.TextGrid'))
if (file.exists(fname)) {
# Prepare text grids
txtgrds <- readtextgrid::read_textgrid(fname)
# Initialize a list to store the annotations for each tier
annotations <- list()
# Loop through the tiers you want to extract
for (tier_name in tiers_to_extract) {
# Filter for the current tier
txtgrd_tier <- txtgrds[txtgrds$tier_name == tier_name, ]
# Extract necessary information
xmin <- round(txtgrd_tier$xmin * 1000)
xmax <- round(txtgrd_tier$xmax * 1000)
anno <- txtgrd_tier$text
anno_num <- txtgrd_tier$annotation_num
# Create the annotation data frame
annotation <- data.frame(xmin, xmax, anno, anno_num)
# Add it to the list with a consistent name
annotations[[tier_name]] <- annotation
}
# Merge envelope and F0 data
ac_merged <- merge(x = env, y = F0, by.x = "time_ms", by.y = "time_ms", all = TRUE)
ac_merged$env <- na.approx(ac_merged$env, x = ac_merged$time_ms, na.rm = FALSE)
# Filter rows where F0 is NA
ac_merged <- ac_merged[(!is.na(ac_merged$F0)),]
# Also remove trailing NA's
ac_merged <- na.trim(ac_merged)
# Loop through the tiers you want to extract and load them into ac_merged
for (tier_name in tiers_to_extract) {
if (tier_name == "Syll") {
# If the tier is "Syll," load numeric annotations
ac_merged$Syll_num <- load.in.anno_num(time_original = ac_merged$time_ms, numeric_annotations = annotations[[tier_name]])
# Also use load.in.event for "Syll" to store annotations
ac_merged[[tier_name]] <- load.in.event(time_original = ac_merged$time_ms, annotations = annotations[[tier_name]])
} else {
# For other tiers, use load.in.event
ac_merged[[tier_name]] <- load.in.event(time_original = ac_merged$time_ms, annotations = annotations[[tier_name]])
}
}
# Add a "File" column with the value of "trial"
ac_merged$File <- trial
# Add some info for analyses
ac_merged <- merge(ac_merged, META[, c("File", cols_to_load)], by = "File", all.x = TRUE)
# Check if the file already exists in the datamerged directory
outputFileName <- paste0(trial, '.csv')
outputFile <- file.path(datamerged, outputFileName)
if (!file.exists(outputFile)) {
# Save to the merge folder
write.csv(ac_merged, outputFile)
} else {
cat("File", outputFileName, "already exists in datamerged directory. Skipping.\n")
}
# Remove unnecessary dependencies
rm(env, F0, txtgrds, annotations, ac_merged, annotation, anno, anno_num, d, fname, outputFile, outputFileName, envelopedata, tier_name, trial, xmin, xmax, wavFile, wavFiles, txtgrd_tier)
}
}
rm(tiers_to_extract, cols_to_load)
# Get a list of all CSV files in the datamerged directory
csv_files <- list.files(datamerged, pattern = ".csv", full.names = TRUE)
# Initialize an empty list to store results
results_list <- list()
# Create a custom function to calculate the mode:
# The most frequent value within an interval
mode_function <- function(x) {
uniq_x <- na.omit(x)  # Remove NAs
if (length(uniq_x) == 0) {
return(NA)  # Return NA if all values are NA
}
counts <- table(uniq_x)
max_count <- max(counts)
mode_val <- names(counts[counts == max_count])
if (is.na(mode_val) && sum(is.na(x)) > max_count) {
return(NA)  # Return NA if NAs are more frequent
}
return(mode_val)
}
# Loop through each CSV file and process it
for (csv_file in csv_files) {
# Read the CSV file
test <- read.csv(csv_file)
# Filter and summarize Syll
test_intervals <-
test %>%
dplyr::filter(!is.na(Syll) & Syll != "") %>%
group_by(Syll_num) %>%
summarize(min_time = min(time_ms),
max_time = max(time_ms)) %>%
arrange(min_time)
# Check if test_intervals has rows
if (nrow(test_intervals) > 0) {
# Calculate additional metrics
test_intervals <- test_intervals %>%
group_by(Syll_num) %>%
mutate(
F0_min = min(test$F0[test$time_ms >= min_time &
test$time_ms <= max_time &
test$F0 > 0], na.rm = TRUE),
F0_max = max(test$F0[test$time_ms >= min_time &
test$time_ms <= max_time &
test$F0 > 0], na.rm = TRUE),
F0_mean = mean(test$F0[test$time_ms >= min_time &
test$time_ms <= max_time &
test$F0 > 0], na.rm = TRUE),
F0_range = ifelse(is.na(F0_min) | is.na(F0_max), NA, F0_max - F0_min),
env_min = min(test$env[test$time_ms >= min_time &
test$time_ms <= max_time], na.rm = TRUE),
env_max = max(test$env[test$time_ms >= min_time &
test$time_ms <= max_time], na.rm = TRUE),
env_mean = mean(test$env[test$time_ms >= min_time &
test$time_ms <= max_time], na.rm = TRUE),
env_range = ifelse(is.na(env_min) | is.na(env_max), NA, env_max - env_min),
duration = ifelse(is.na(min_time) | is.na(max_time), NA, max_time - min_time),
Prosodic_Prom =
mode_function(test$Prosodic_Prom[test$time_ms >= min_time &
test$time_ms <= max_time]),
TP =
mode_function(test$TP[test$time_ms >= min_time &
test$time_ms <= max_time]),
Word =
mode_function(test$Word[test$time_ms >= min_time &
test$time_ms <= max_time]),
Syll_num = unique(test$Syll_num[test$time_ms >= min_time &
test$time_ms <= max_time &
!is.na(test$Syll_num)])
) %>%
left_join(test %>%
select(File, Participant, Language, Item_type, Item_num, Focus, Gender, Age, Syll, Syll_num) %>%
distinct(File, Syll_num, .keep_all = TRUE), by = "Syll_num")
# Append the result to the list
results_list[[csv_file]] <- test_intervals
}
}
View(test_intervals)
View(tg)
View(results_list)
# Get a list of all CSV files in the datamerged directory
csv_files <- list.files(datamerged, pattern = ".csv", full.names = TRUE)
# Initialize an empty list to store results
results_list <- list()
mode_function <- function(x) {
uniq_x <- na.omit(x)  # Remove NAs
if (length(uniq_x) == 0) {
return(NA)  # Return NA if all values are NA
}
counts <- table(uniq_x)
max_count <- max(counts)
mode_vals <- names(counts[counts == max_count])
if (length(mode_vals) > 1) {
mode_val <- mode_vals[1]  # Select the first mode in case of ties
} else {
mode_val <- mode_vals
}
if (is.na(mode_val) && sum(is.na(x)) > max_count) {
return(NA)  # Return NA if NAs are more frequent
}
return(mode_val)
}
# Loop through each CSV file and process it
for (csv_file in csv_files) {
# Read the CSV file
test <- read.csv(csv_file)
# Filter and summarize Syll
test_intervals <-
test %>%
dplyr::filter(!is.na(Syll) & Syll != "") %>%
group_by(Syll_num) %>%
summarize(min_time = min(time_ms),
max_time = max(time_ms)) %>%
arrange(min_time)
# Check if test_intervals has rows
if (nrow(test_intervals) > 0) {
# Calculate additional metrics
test_intervals <- test_intervals %>%
group_by(Syll_num) %>%
mutate(
F0_min = min(test$F0[test$time_ms >= min_time &
test$time_ms <= max_time &
test$F0 > 0], na.rm = TRUE),
F0_max = max(test$F0[test$time_ms >= min_time &
test$time_ms <= max_time &
test$F0 > 0], na.rm = TRUE),
F0_mean = mean(test$F0[test$time_ms >= min_time &
test$time_ms <= max_time &
test$F0 > 0], na.rm = TRUE),
F0_range = ifelse(is.na(F0_min) | is.na(F0_max), NA, F0_max - F0_min),
env_min = min(test$env[test$time_ms >= min_time &
test$time_ms <= max_time], na.rm = TRUE),
env_max = max(test$env[test$time_ms >= min_time &
test$time_ms <= max_time], na.rm = TRUE),
env_mean = mean(test$env[test$time_ms >= min_time &
test$time_ms <= max_time], na.rm = TRUE),
env_range = ifelse(is.na(env_min) | is.na(env_max), NA, env_max - env_min),
duration = ifelse(is.na(min_time) | is.na(max_time), NA, max_time - min_time),
Prosodic_Prom =
mode_function(test$Prosodic_Prom[test$time_ms >= min_time &
test$time_ms <= max_time]),
TP =
mode_function(test$TP[test$time_ms >= min_time &
test$time_ms <= max_time]),
Word =
mode_function(test$Word[test$time_ms >= min_time &
test$time_ms <= max_time]),
Syll_num = unique(test$Syll_num[test$time_ms >= min_time &
test$time_ms <= max_time &
!is.na(test$Syll_num)])
) %>%
left_join(test %>%
select(File, Participant, Language, Item_type, Item_num, Focus, Gender, Age, Syll, Syll_num) %>%
distinct(File, Syll_num, .keep_all = TRUE), by = "Syll_num")
# Append the result to the list
results_list[[csv_file]] <- test_intervals
}
}
rm(csv_file, csv_files, test, test_intervals)
# Combine all results into a single dataframe
final_result <- do.call(rbind, results_list)
rm(final_result)
rm(csv_file, csv_files, test, test_intervals)
# Combine all results into a single dataframe
final_result <- do.call(rbind, results_list)
# Rearrange the column names
final_result <- final_result %>%
select(
"File", "Language", "Participant", "Item_type", "Item_num", "Focus", "TP", "Word", "Syll", "Syll_num", "Prosodic_Prom",
"min_time", "max_time", "F0_mean", "F0_min", "F0_max", "F0_range", "env_mean", "env_min", "env_max", "env_range", "duration", "Gender", "Age"
)
df <- final_result %>%
filter(Item_type == "target") #%>%
df <- final_result %>%
dplyr::filter(Item_type == "target") #%>%
View(df)
# Save 'df' as a CSV file
write.csv(df, paste0(data), row.names = FALSE)
data
# Save 'df' as a CSV file
write.csv(df, "C:/Users/cwiek/Documents/GitHub/Prominence_in_focus/Analysis/MultIS_data/", row.names = FALSE)
# Save 'df' as a CSV file
write.csv(df, "C:/Users/cwiek/Documents/", row.names = FALSE)
# Save 'df' as a CSV file
write.csv(df, paste0(data), row.names = FALSE)
# Save 'df' as a CSV file
write.csv(df, paste0(data, "df.csv"), row.names = FALSE)
